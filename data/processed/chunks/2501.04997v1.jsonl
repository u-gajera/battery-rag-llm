{"chunk_id": "2501.04997v1_p0_c0", "doc_id": "2501.04997v1", "text": "GINET: Integrating Sequential and Context-Aware Learning for Battery Capacity Prediction Sara Sameer Singapore Institute of Technology Singapore, 828608 sara.sameer@singaporetech.edu.sg Wei Zhang∗ Singapore Institute of Technology Singapore, 828608 wei.zhang@singaporetech.edu.sg Xin Lou Singapore Institute of Technology Singapore, 828608 xin.lou@singaporetech.edu.sg Qingyu Yan Nanyang Technological University Singapore, 639798 alexyan@ntu.edu.sg Terence Goh ST Engineering Singapore, 619523 kenglengterence.goh@stengg.com Yulin Gao ST Engineering Singapore, 619523 gao.yulin@stengg.com Abstract—The surging demand for batteries requires advanced battery management systems, where battery capacity modelling is a key functionality. In this paper, we aim to achieve ac- curate battery capacity prediction by learning from historical measurements of battery dynamics. We propose GINET, a gated recurrent units enhanced Informer network, for predicting battery’s capacity. The novelty and competitiveness of GINET lies in its capability of capturing sequential and contextual information from raw battery data and reflecting the battery’s complex behaviors with both temporal dynamics and long-term dependencies. We conducted an experimental study based on a publicly available dataset to showcase GINET’s strength of gaining a holistic understanding of battery behavior and pre- dicting battery capacity accurately. GINET achieves 0.11 mean absolute error for predicting the battery capacity in a sequence of future time slots without knowing the historical battery capacity. It also outperforms the latest algorithms significantly with 27% error reduction on average compared to Informer. The promising results highlight the importance of customized and optimized integration of algorithm and battery knowledge and shed light on other industry applications as well. Index Terms—battery capacity, state of charge, machine learn- ing, industrial artificial intelligence I. INTRODUCTION Batteries power devices and systems for different industry sectors, from consumer electronics to mobility. They are often considered the cornerstone of modern technology and society. The demand for Li-ion batteries alone is expected to grow by over 30% annually to reach ∼4.7 TWh by 2030. Batteries for mobility, e.g., electric vehicles (EVs), account for about 90% of the demand [1]. In line with the surging demand, the revenue of the battery value chain is expected to surpass 400 billion dollars by 2030. The huge demand and broad adoption require batteries to be safe, efficient, and sustainable, typically This", "page": 0, "position": 0}
{"chunk_id": "2501.04997v1_p0_c1", "doc_id": "2501.04997v1", "text": "Li-ion batteries alone is expected to grow by over 30% annually to reach ∼4.7 TWh by 2030. Batteries for mobility, e.g., electric vehicles (EVs), account for about 90% of the demand [1]. In line with the surging demand, the revenue of the battery value chain is expected to surpass 400 billion dollars by 2030. The huge demand and broad adoption require batteries to be safe, efficient, and sustainable, typically This work was supported in part by SIT’s Ignition Grant (STEM) (Grant ID: IG (S) 2/2023 – 792), A*STAR under its MTC Programmatic (Award M23L9b0052), MTC Individual Research Grants (IRG) (Award M23M6c0113), and the National Research Foundation, Singapore and In- focomm Media Development Authority under its Future Communications Research & Development Programme (Grant FCP-SIT-TG-2022-007). ∗Corresponding author. Email: wei.zhang@singaporetech.edu.sg. ensured by the critical battery management system (BMS). A BMS shall actively monitor battery dynamics, e.g., current, voltage, and temperature, and guard against overcharging, deep discharge, thermal runaway, etc., that can result in hazards like fires and explosions [2]. Often, BMS makes decisions based on several important indicators. One of them is state of charge (SoC), which tells the remaining capacity of a battery as the ratio of its total capacity at any given time [3]. SoC is highly important for battery management, especially in applications like EVs and smart grids. Accurate measurement of SoC has been a challenging task. Traditional methods include open-circuit voltage [4], Coulomb-counting [5], etc. These methods are simple and widely used, though they face challenges with limited accuracy and the inability to adapt to dynamic battery conditions. For example, [4] requires batteries to be at rest for accurate measurements and is impractical for applications during opera- tion time. Data-driven approaches based on machine learning (ML) have gained more significance in recent years due to their high accuracy and robustness. Battery measurements are often sequential and naturally time-series ML algorithms, e.g., recurrent neural networks (RNNs), are used for SoC estimation and prediction. Long short-term memory (LSTM) is perhaps the most popular type of RNN and serves as a baseline in many SoC studies. Newer RNNs have been investigated recently", "page": 0, "position": 1}
{"chunk_id": "2501.04997v1_p0_c2", "doc_id": "2501.04997v1", "text": "Data-driven approaches based on machine learning (ML) have gained more significance in recent years due to their high accuracy and robustness. Battery measurements are often sequential and naturally time-series ML algorithms, e.g., recurrent neural networks (RNNs), are used for SoC estimation and prediction. Long short-term memory (LSTM) is perhaps the most popular type of RNN and serves as a baseline in many SoC studies. Newer RNNs have been investigated recently to improve SoC prediction performance and a comparison study is available in [6]. The latest Transformer-based ML is also capable of handling sequential data but in a differ- ent way than RNNs, e.g., with attention mechanisms. The Transformer models have demonstrated superior performance to traditional RNNs for many applications [7]. In a recent study [8], a temporal transformer-based network is used to model the relationship between battery measurements and SoC. A common feature of most ML-based SoC studies is that each uses a single ML algorithm. Battery behavior is known to be complex, diverse, and multi-faceted, and single-model- based SoC prediction is often insufficient to understand battery behavior well and deliver optimal performance. arXiv:2501.04997v1 [cs.LG] 9 Jan 2025", "page": 0, "position": 2}
{"chunk_id": "2501.04997v1_p1_c0", "doc_id": "2501.04997v1", "text": "SoC models based on multiple ML algorithms are promising to capture the battery behavior complexity by leveraging different algorithms’ complementary strengths. In [9], LSTM is integrated with a convolutional neural network (CNN) to estimate SoC. However, the idea is to process battery data of different modalities instead of focusing on the application and exploiting complex battery dynamics. Looking into the battery capacity, there are different aspects of the battery behaviors and operating conditions. Naturally, there is se- quential information in the battery data, in time series. The information is related to the battery’s temporal dynamics, e.g., gradual capacity decline during short-term discharging within a cycle and long-term battery degradation across cycles. There also exists contextual information and battery capacity related external factors, such as ambient temperature, fast charging, high discharge rates, and frequent deep discharge. These factors affect battery capacity in ways that are not fully time-dependent, or sequential. Capturing such contextual information complements sequential learning to gain a holistic understanding of battery behavior and accordingly, predict SoC accurately. In this paper, we aim to integrate sequential and context- aware learning for battery capacity prediction. RNNs are natural options to process sequential information. Specifically, we employ gated recurrent units (GRU), a variant of LSTM with a simplified architecture and enhanced generalizability [10]. Contextual information is not always temporal and we apply Transformer models to capture long-range dependencies with their powerful attention mechanisms. Informer [11], one of the latest Transformer architectures, is used in this study. Informer has efficient attention and sequence distillation mech- anisms that make it suitable for processing long-sequence data and extracting contextual information effectively. Accordingly, we propose GINET, a GRU-enhance Informer network for battery capacity prediction. GINET configures Informer to be the core architecture for battery analytics. We customize and optimize Informer’s embedding layer with GRU-generated fea- tures and feature fusion to provide enhanced embedding with both sequential and contextual information. The information is analyzed by Informer’s encoder and decoder to generate battery SoC prediction. In summary, we have the following main contributions in this paper. • We designed and developed GINET, a GRU-enhanced In- former architecture", "page": 1, "position": 0}
{"chunk_id": "2501.04997v1_p1_c1", "doc_id": "2501.04997v1", "text": "configures Informer to be the core architecture for battery analytics. We customize and optimize Informer’s embedding layer with GRU-generated fea- tures and feature fusion to provide enhanced embedding with both sequential and contextual information. The information is analyzed by Informer’s encoder and decoder to generate battery SoC prediction. In summary, we have the following main contributions in this paper. • We designed and developed GINET, a GRU-enhanced In- former architecture for battery capacity prediction based on historical battery monitoring data. • We customized and optimized the integration GRU and Informer to capture both sequential and contextual fea- tures of batteries effectively with feature fusion. • We conducted an experimental study and showed that GINET predicts battery SoC accurately with a minimal mean absolute error (MAE) of 0.11 and achieves 76% and 27% performance improvement compared to GRU and Informer, respectively. The rest of the paper is organized as follows. We present GINET for battery capacity prediction in Section II. In Section GINET Data Pre-processing Input Window Forecast Horizon Normalized GRU Sequential Features Informer Embedding Feature Fusion Encoder Embedding Decoder Embedding Distillation ProbSparse ProbSparse Encoder Attention Masked ProbSparse Projection Layer SoC Prediction in Forecast Horizon Battery Input Temporal Positional Current Voltage Temp. Decoder Fig. 1. An illustration of GINET architecture. GINET performs data pre- processing for the battery time-series data. GRU is used to capture sequential information and extract corresponding features, and Informer is employed to model long-term dependence and extract contextual information. Supporting modules include feature fusion, output mapping to forecast horizon, etc. III, we conduct an experimental study and present results and discussions. Finally, we conclude this paper and suggest future works in Section IV. II. GINET METHODOLOGY We present the overall architecture and main components of GINET in this section. An illustration of the GINET architecture is in Fig. 1. A. Model Overview and Technology Recap GINET uses the battery monitoring data for battery ca- pacity prediction. The collected raw data undergoes data pre-processing to generate input vectors ready for GINET prediction. The input of GINET is first processed by a GRU module to extract sequential features and", "page": 1, "position": 1}
{"chunk_id": "2501.04997v1_p1_c2", "doc_id": "2501.04997v1", "text": "present the overall architecture and main components of GINET in this section. An illustration of the GINET architecture is in Fig. 1. A. Model Overview and Technology Recap GINET uses the battery monitoring data for battery ca- pacity prediction. The collected raw data undergoes data pre-processing to generate input vectors ready for GINET prediction. The input of GINET is first processed by a GRU module to extract sequential features and understand time- series patterns from the battery data. The GRU features are fused with the original feature to enhance the sequential features with contextual information on battery dynamics. The fused features serve as the input of the Informer’s embedding and Informer analyzes the features with its attention-based encoder and decoder. Finally, GINET maps the output of the Informer module to the battery capacity forecast horizon and produces the SoC prediction results accordingly. Before presenting GINET’s technology details, we provide a brief recap of its supporting ML technologies, GRU and Informer, as below. 1) GRU: GRU is one type of RNN. Compared to the popular RNN-based LSTM, GRU’s structure is simple. GRU has one less gate than LSTM. It combines LSTM’s forget and input gates into an update gate to control the amount of historical information to be retained and new information to be used. It also has a reset gate to control the historical information to be deleted and GRU’s unique gate mechanism minimizes gradient vanishing issues that are common for other", "page": 1, "position": 2}
{"chunk_id": "2501.04997v1_p2_c0", "doc_id": "2501.04997v1", "text": "RNNs. Finally, the gate output is used to calculate the final hidden state, which is the GRU-based representation of the input data, e.g., GRU-based battery temporal dependencies. GRU details such as gates and hidden state calculations are available in [10]. 2) Informer: Transformers have been widely used for time- series forecasting applications and many of them are based on full attention as, Attention(Q, K, V ) = softmax \u0010 QKT / √ d \u0011 V (1) where Q, K, and V are query, key, and value matrices, respectively, and d is the input dimension. A key challenge of Eq. (1) is that the attention weights of all pairs of queries and keys are computed with a high time complexity of O(n2) with an input length of n. Informer aims to process long sequence data effectively. Its key innovation is to reduce com- putation complexity with a new attention mechanism called ProbSparse self-attention as, ProbSparse(Q, K, V ) = softmax \u0010 QKT / √ d \u0011 V (2) where Q is a sparse matrix which contains top-u queries with dominant attention only and u is expected to be much smaller than n. The dominance of each query key pair is quantified by a query sparsity measurement function M(q, K) for a query q. The new attention allows the Informer to achieve a time complexity of O(n log n). Furthermore, Informer uses distillation operations to produce shortened representations of long sequences. Overall, Informer with its attention-based encoder and decoder processes data input and performs linear transformations for forecast generation. Informer details such as sparsity measurements can be found in [11]. B. Building Blocks Based on the model overview, we decompose GINET archi- tecture into several building blocks and present the technical details below. 1) Data Pre-processing: Most ML algorithms favor big data for both quantity and diversity to achieve optimal perfor- mance. However, big data may not be practical for real-world applications like battery capacity prediction, as it requires increased costs for data collection, storage, processing, etc. In this paper, we aim to develop a practical solution for batteries and we consider", "page": 2, "position": 0}
{"chunk_id": "2501.04997v1_p2_c1", "doc_id": "2501.04997v1", "text": "tecture into several building blocks and present the technical details below. 1) Data Pre-processing: Most ML algorithms favor big data for both quantity and diversity to achieve optimal perfor- mance. However, big data may not be practical for real-world applications like battery capacity prediction, as it requires increased costs for data collection, storage, processing, etc. In this paper, we aim to develop a practical solution for batteries and we consider common battery measurements including voltage, current, and temperature that are easy to monitor for GINET processing. Nevertheless, we would like to mention that GINET is not restricted to specific features, and its input can be customized to accommodate different battery systems. Specifically, let xt = (xI t , xV t , xT t ) be the battery data at time slot t where I, V, and T represent current, voltage, and temperature, respectively. GINET is capable of processing the data of a sequence of time slots, i.e., time-series, and we apply over-lapping windows [8] to prepare the input data of GINET. Let T in be the length of the input window and the input of GINET is Xt = (xt−T in, xt−T in+1, . . . , xt−1) at time slot t. GINET can estimate the battery capacity of the current time slot as well as future time slots, referred to as forecast horizon. Let T out be the length of the forecast horizon and ˆyt be GINET’s estimated SoC for time t. With the input of T in time slots data, GINET produces ˆYt = (ˆyt, ˆyt+1, . . . , ˆyt+T out−1) for T out time slots at time t. For simplicity, let ˆySoC t = Pt+T out−1 i=t ˆyi/T out be the average of the T out estimations and our reported results in this paper are based on ˆySoC t by default. Battery capacity datasets often provide the ground- truth SoC as Yt = (yt, yt+1, . . . , yt+T out−1) for evaluating the performance of battery capacity models. However, the ground truth cannot be used during model training and is unavailable in many practical settings.", "page": 2, "position": 1}
{"chunk_id": "2501.04997v1_p2_c2", "doc_id": "2501.04997v1", "text": "i=t ˆyi/T out be the average of the T out estimations and our reported results in this paper are based on ˆySoC t by default. Battery capacity datasets often provide the ground- truth SoC as Yt = (yt, yt+1, . . . , yt+T out−1) for evaluating the performance of battery capacity models. However, the ground truth cannot be used during model training and is unavailable in many practical settings. Furthermore, we employ min-max normalization for input feature scaling and keep the output non-normalized to align with real-world SoC prediction. 2) GRU-enhanced Features and Feature Fusion: At each time slot, we extract features from the GINET input and expect the features to be better correlated with battery capacity than the raw input. We aim to capture both temporal dependencies and contextual features. In GINET, we employ GRU to extract temporal dependencies from the sequential battery data input. Specifically, GRU processes the sequential input over a se- quence of time slots and generates high-dimensional (1024 in this paper) hidden state representation at each time slot. We configure two layers for hierarchical feature extraction and representation where the output of the first layer serves as the input of the second layer and a dropout layer is applied after the first layer to prevent overfitting. The representations effec- tively summarize the temporal dependencies from the battery time series and encode complex interactions between battery dynamics over time. Finally, the representations are passed through a linear transformation to map the representations to feature space that matches the original input and let GRU’s generated feature be Ht = GRU(Xt). Such mapping ensures compatibility for the following feature fusion. 3) Feature Fusion: We apply an element-wise addition between the GRU-generated features and the GINET’s original input, and the fused feature is Ft = Ht + Xt for time t. We expect the fused features to retain contextual features from the original input while embedding temporal features extracted by GRU, and serve as the input of the Informer-based data modeling detailed below. 4) Informer-based SoC Prediction: The fused feature Ft is passed to the Informer’s embedding layer", "page": 2, "position": 2}
{"chunk_id": "2501.04997v1_p2_c3", "doc_id": "2501.04997v1", "text": "addition between the GRU-generated features and the GINET’s original input, and the fused feature is Ft = Ht + Xt for time t. We expect the fused features to retain contextual features from the original input while embedding temporal features extracted by GRU, and serve as the input of the Informer-based data modeling detailed below. 4) Informer-based SoC Prediction: The fused feature Ft is passed to the Informer’s embedding layer first. Ft is converted into a high-dimensional space suitable for Informer’s attention mechanism. We use three different embedding techniques, including positional, value, and temporal, to enrich the time- series battery data. The first one helps capture the position of each data point of the input and value embedding uses a 1-dimensional convolutional layer to expand the features into the desired embedding dimension. The last one encodes time- related features, e.g., seconds, to capture the potential periodic nature of batteries. Finally, the input from all three embedding techniques is unified into a single data embedding layer. The following stage of Informer processing is encoding. The key component of encoding is the multi-head self-attention which incorporates ProbSparse attention for processing", "page": 2, "position": 3}
{"chunk_id": "2501.04997v1_p3_c0", "doc_id": "2501.04997v1", "text": "long sequence data efficiently. The process efficiency can be further improved with a distillation mechanism that reduces sequence length with convolutional layers. Another component is the feed-forward layer, which expands GINET’s capacity to learn complex transformations by introducing non-linearity through activation functions like ReLU, further refining the feature space. Overall, the encoder-generated features are expected to be rich in temporal and contextual information, ready to be processed by the decoder for battery analytics. In the decoding stage, a unique design is to combine the decoder’s input and the encoder’s output. The decoder first utilizes its own data along with a placeholder sequence repre- senting forecast horizon to perform multi-head ProbSparse self-attention operation. Then, it integrates the encoder’s in- termediate output where the decoder aligns its predictions with the feature representations extracted by the encoder. This step ensures that the decoder leverages both historical depen- dencies as well as the temporal and contextual information learned during encoding. Finally, the output of the decoder is passed to a fully connected layer to adjust the dimension of the output to the prediction horizon of battery capacity as ˆYt = Informer(Ft) at time t. Overall, GINET infuses GRU into Informer, where GRU captures fine-grained temporal patterns to complement In- former’s attention-based long sequence modeling. Such syn- ergy is essential for accurate battery capacity prediction with battery dynamics being both temporal and contextual. III. EXPERIMENTAL STUDY We present our experimental study in this section for experiment setup, results presentation, and discussions. A. Data A battery capacity model shall be trained and optimized with a real-world battery dataset. For our experimental study, we use an open dataset [12]. The dataset is based on Panasonic 18650PF Li-ion batteries and tests were conducted across a wide temperature range, i.e., −20 to 25 °C, to analyze battery performance variations under different ambient conditions. The key tests involved in this dataset include charging and discharging batteries at a slow and steady rate (C/20 charge- discharge cycles), measuring battery power and energy capa- bilities (hybrid pulse power characterization), and analyzing the internal resistances of the cells (electrochemical impedance spectroscopy). The tests", "page": 3, "position": 0}
{"chunk_id": "2501.04997v1_p3_c1", "doc_id": "2501.04997v1", "text": "batteries and tests were conducted across a wide temperature range, i.e., −20 to 25 °C, to analyze battery performance variations under different ambient conditions. The key tests involved in this dataset include charging and discharging batteries at a slow and steady rate (C/20 charge- discharge cycles), measuring battery power and energy capa- bilities (hybrid pulse power characterization), and analyzing the internal resistances of the cells (electrochemical impedance spectroscopy). The tests also simulated the battery perfor- mance of a Ford F150 electric truck under different driving conditions, e.g., aggressive urban driving (US06), highway driving (HWFET), typical urban driving (UDDS), and tailored driving profiles generated by a neural network. The dataset provides high-resolution (10 measurements per second) battery dynamics, including voltage, current, power, amp-hours, watt- hours, and battery temperature. Overall, this data offers a ground for our experimental study with diverse operational conditions for model development and validation. With the dataset, we parsed, concatenated, normalized, and split the data into training, validation, and test datasets, with a ratio of 10:2:5. The training data is composed of mixed cycles and driving profiles at four different ambient temperatures (i.e., −10, 0, 10, and 25 °C). Two cycles are reserved for testing to evaluate model performance with unseen data. B. Model Configuration and Implementation GINET involves several parameters. We have optimized the parameters and chosen one of the optimal configurations as the default setting. The batch size is 32, and the learning rate is 1e−4. A scheduler for learning rate decrease, zero- padding, and distillation are set to true. We have 20 epochs and early stopping can be triggered when the model stops improving. Battery capacity modelling is relatively a small- scale application, and we configure 2 encoder layers and 1 decoder layer in GINET. We implement GINET in the PyTorch library. All experiments are conducted in our workstation with an AMD Ryzen 9 5950X processor and NVIDIA GTX 3080 GPU. We present experiment results based on statistical perfor- mance. Among different performance evaluation metrics, we choose two mainstream ones including MAE and root mean squared error (RMSE). Relatively, RMSE is more susceptible to outliers", "page": 3, "position": 1}
{"chunk_id": "2501.04997v1_p3_c2", "doc_id": "2501.04997v1", "text": "configure 2 encoder layers and 1 decoder layer in GINET. We implement GINET in the PyTorch library. All experiments are conducted in our workstation with an AMD Ryzen 9 5950X processor and NVIDIA GTX 3080 GPU. We present experiment results based on statistical perfor- mance. Among different performance evaluation metrics, we choose two mainstream ones including MAE and root mean squared error (RMSE). Relatively, RMSE is more susceptible to outliers than MAE because the mistakes are first squared. Let ˆySoC = {ˆySoC 1 , . . . , ˆySoC n } be a list of n SoC predictions and let ySoC = {ySoC 1 , . . . , ySoC n } be the corresponding ground- truth SoC. MAE is given by, MAE = 1 n Xn k=1 ˆySoC k −ySoC k , (3) and RMSE can be calculated as, RMSE = r 1 n Xn k=1 \u0000ˆySoC k −ySoC k \u00012. (4) In the following parts, we show that our proposed GINET achieves minimal MAE and RMSE for SoC prediction. C. Comparison Study We first demonstrate GINET’s performance competitiveness by comparing GINET with three comparison algorithms, in- cluding LSTM, GRU, and Informer. We have implemented comparison algorithms with parameters optimized and we consider different settings for a fair comparison. Specifically, we investigate the algorithms’ performance with an input window of 10, 100, and 200 and a forecast horizon of 10 and 25. We report the performance in terms of MAE and RMSE and the results are shown in Table I. We have the following observations. 1) Overall Results: First, GINET performs the best. In nearly all tested configurations of input window and forecast horizon, as well as for different metrics, GINET outperforms the comparison algorithms. GRU and Informer are two build- ing blocks of GINET, and we notice that applying either GRU or Informer alone is insufficient for achieving optimal performance. This highlights GINET’s concept of integrating sequential and context information, handled by GRU and Informer, respectively, for battery capacity prediction. Rel- atively, Informer alone performs better than GRU, and this", "page": 3, "position": 2}
{"chunk_id": "2501.04997v1_p4_c0", "doc_id": "2501.04997v1", "text": "TABLE I A COMPARISON STUDY OF GINET AND COMPARISON ALGORITHMS INCLUDING LSTM, GRU, AND INFORMER FOR BATTERY CAPACITY PREDICTION. VARIOUS COMBINATIONS OF INPUT WINDOW AND FORECAST HORIZON HAVE BEEN TESTED AND THE RESULTS BASED ON MAE AND RMSE ARE REPORTED. THE PERFORMANCE IMPROVEMENT IN PERCENTAGE WITH LSTM AS THE BASELINE SHOWS GINET’S SIGNIFICANT ERROR REDUCTION FOR SOC PREDICTION. Forecast Horizon 10 25 10 Input Window 10 100 200 10 100 200 200 SoC Model MAE Impr. LSTM 0.33 0.27 0.26 0.36 0.27 0.26 − GRU 0.25 0.25 0.24 0.27 0.25 0.24 7.7 Informer 0.18 0.17 0.17 0.20 0.18 0.18 34.6 GINET (ours) 0.15 0.14 0.11 0.19 0.15 0.13 57.7 SoC Model RMSE Impr. LSTM 0.42 0.30 0.29 0.43 0.31 0.30 − GRU 0.29 0.28 0.26 0.30 0.29 0.27 10.3 Informer 0.21 0.21 0.19 0.22 0.21 0.20 34.5 GINET (ours) 0.17 0.16 0.14 0.22 0.18 0.15 51.7 demonstrates the superior modelling capability of the latest Transformer architecture. For sequential data analysis, LSTM is one of the most popular algorithms. Both LSTM and GRU are RNNs where GRU’s architecture is simpler with fewer gates and parameters. We can see that LSTM incurs higher errors than GRU and the reason should be that GRU’s simple architecture aligns well with our application, which is relatively small-scale with over-fitting risks. Let LSTM be the baseline algorithm. We calculate the error reduction improvement for the rest algorithms with input window 200 and forecast horizon 10. We can see that GRU and Informer outperform LSTM with 8% and 35% lower MAE, respectively. GINET has the best performance and the improvement is 58%, which is 46% and 28% better than its building blocks GRU and Informer, respectively. RMSE results support our observations as well. 2) Input Window: A large input window helps improve performance. Many ML models favor more data to establish an accurate mapping from the input data to the output. For battery capacity prediction, the prediction output is correlated with not only short-term historical battery operation data but also long- term data. GINET employs sequential learning and context- aware learning for short-term and long-term data, respectively.", "page": 4, "position": 0}
{"chunk_id": "2501.04997v1_p4_c1", "doc_id": "2501.04997v1", "text": "results support our observations as well. 2) Input Window: A large input window helps improve performance. Many ML models favor more data to establish an accurate mapping from the input data to the output. For battery capacity prediction, the prediction output is correlated with not only short-term historical battery operation data but also long- term data. GINET employs sequential learning and context- aware learning for short-term and long-term data, respectively. Given a large input window, our tested algorithms have more information to process and can achieve improved performance if the information is processed well. For GINET with forecast horizon 10, the achieved MAE is 0.15, 0.14, and 0.11 for input window 10, 100, and 200, respectively, with a significant MAE improvement of 27% from window 10 to 200. LSTM also shows a 21% improvement from window 10 to 200 for MAE. Interestingly, we observe that GRU or Informer alone is much less sensitive to the input window size compared to GINET, where the difference of MAE is around 0.01 only for forecast horizon 10. This implies the importance of dealing with both TABLE II GINET’S PERFORMANCE SENSITIVITY TO DIFFERENT INFORMER CONFIGURATIONS FOR ATTENTION MECHANISM AND DISTILLATION. THE RESULTS ARE GENERATED WITH INPUT WINDOW 100 AND FORECAST HORIZON 25 AND BOTH MAE AND RMSE RESULTS ARE REPORTED. GINET WITH PR O BSP A R S E-BASED ATTENTION AND DISTILLATION ENABLED HAS THE OPTIMAL PERFORMANCE FOR SOC PREDICTION. Attention Mechanism w/ distillation w/t distillation MAE RMSE MAE RMSE ProbSparse 0.15 0.18 0.16 0.19 Full Attention 0.16 0.19 0.18 0.21 short-term and long-term data with different technologies to well explore the potential of large window input data. 3) Forecast Horizon: Besides, predicting for a long horizon is challenging. Compared to horizon 10, both MAE and RMSE results of the tested algorithms for horizon 25 are generally higher. For GINET with input window 200, its MAE is 18% higher with the forecast increased from 10 to 25. Intuitively, the historical battery data is more correlated to the near future battery dynamics and the long-term changes depend on the up- dated battery operation information which", "page": 4, "position": 1}
{"chunk_id": "2501.04997v1_p4_c2", "doc_id": "2501.04997v1", "text": "a long horizon is challenging. Compared to horizon 10, both MAE and RMSE results of the tested algorithms for horizon 25 are generally higher. For GINET with input window 200, its MAE is 18% higher with the forecast increased from 10 to 25. Intuitively, the historical battery data is more correlated to the near future battery dynamics and the long-term changes depend on the up- dated battery operation information which is not yet available. Given that the prediction accuracy drops for long horizons, it is important to convey the algorithm strength and limitation to users with comprehensive performance measurements, e.g., by quantifying the prediction uncertainty [13]. D. Sensitivity Analysis GINET has several parameters that shall be optimized for optimal performance. In this part, we conduct sensitiv- ity analysis to understand GINET’s performance variation with different parameter settings. Specifically, we investigate GINET’s ProbSparse attention mechanism, distillation, and the number of encoder and decoder layers. 1) Attention: GINET uses ProbSparse attention which addresses the inefficiencies of full attention used in standard Transformer models by focusing on the most critical depen- dencies within the input sequence. In full attention, all data points in a sequence are treated equally and all pairwise interactions are calculated. ProbSparse is different by se- lecting informative interactions only and enables the model to use memory efficiently and focus on the most relevant battery patterns for effective learning. Seen from Table II, ProbSparse helps reduce the prediction error compared to full attention and improve MAE by 6.3% and 5.3% for the implementation with and without distillation, respectively. 2) Distillation: Distillation complements ProbSparse for efficient and effective processing of long battery sequences. While ProbSparse focuses on optimizing attention, distil- lation aims to reduce the length of the data input. It applies convolutional layers to shorten the data sequence in the en- coder without sacrificing temporal and contextual information. With reduced redundancy, the distilled data allows GINET to focus on high-level features for improved generalization and performance on new battery dynamics. Table II shows that distillation helps improve prediction accuracy for both", "page": 4, "position": 2}
{"chunk_id": "2501.04997v1_p5_c0", "doc_id": "2501.04997v1", "text": "10 100 200 Input Window 0.10 0.15 0.20 MAE E1D1 E2D2 E2D1 Fig. 2. The GINET’s achieved MAE with different numbers of encoder and decoder layers. The results for input windows 10, 100, and 200 for forecast horizon 25 are reported. EiDj in legend means i encoder layers and j decoder layers. GINET performs the best with two encoder layers and one decoder layer, i.e., E2D1 in the figure. ProbSparse-based and full attention, where the MAE im- provement is 6.3% and 11.1%, respectively. 3) Encoder Decoder Layers: Encoder and decoder are key components of GINET (refer to Fig. 1). Inside each encoder, there can be multiple layers, each with attention, feedforward layer, distillation, etc., and the multiple layers can be stacked together in the encoder. This is true for the decoder also, except for some differences such as the decoder’s specialized attention. In this part, we test the performance of GINET with different numbers of encoder layers and decoder layers. As battery capacity modelling is relatively a small-scale application, we configure the GINET models with minimal encoder and decoder layers. Specifically, we test three configurations, including one encoder layer and one decoder layer, two encoder layers, and two decoder layers, and two encoder layers and one decoder layer, denoted as E1D1, E2D2, and E2D1, respectively, as shown in Fig. 2. We can see that one encoder layer is insufficient to achieve optimal performance. Compared to the best configuration, i.e., E2D1 with two encoder layers and one decoder layer, the MAE is on average 8.3% higher for one encoder layer only. This suggests that our battery data embedding is not simple enough to be well modeled with one encoder layer. An encoder shall well process GINET’s embedding that contains a comprehensive representation of both original battery data and the GRU-generated temporal dependencies. For the decoder, we notice that one layer supports optimal performance, and increasing the number of decoder layers does not enhance GINET’s prediction, with 10.9% higher MAE on average. A potential reason is that the decoder’s role in GINET is to interpret encoder-generated representations and produce bat- tery capacity", "page": 5, "position": 0}
{"chunk_id": "2501.04997v1_p5_c1", "doc_id": "2501.04997v1", "text": "encoder shall well process GINET’s embedding that contains a comprehensive representation of both original battery data and the GRU-generated temporal dependencies. For the decoder, we notice that one layer supports optimal performance, and increasing the number of decoder layers does not enhance GINET’s prediction, with 10.9% higher MAE on average. A potential reason is that the decoder’s role in GINET is to interpret encoder-generated representations and produce bat- tery capacity predictions and one layer meets the application requirements well. When more layers are configured for the decoder, there could be unnecessary model complexity and risks of overfitting. IV. CONCLUSION In this paper, we propose GINET, a GRU-enhanced In- former network for battery SoC prediction. GINET is designed and developed to capture both sequential and contextual in- formation of battery dynamics where battery capacity changes are correlated with both information. Among the GINET com- ponents, GRU models sequential dependencies and Informer captures contextual information by analyzing long sequence battery data with its attention mechanism. GINET inherits and customizes Informer’s ProbSparse attention and distillation operation for efficient and effective battery data processing. An experimental study is conducted based on a public dataset with Panasonic 18650PF battery cells. We demonstrate that GINET yields a low error rate and stable prediction across various input window and forecast horizon configurations. The minimal achieved MAE for GINET is 0.11 and GINET out- performs comparison algorithms, including LSTM, GRU, and Informer, significantly. The average error reduction compared to the second best performed Informer is 27%. And RMSE results suggest similar insights. In the future, we would like to enhance the extraction of sequential and contextual information with further optimized GRU and Informer or the latest ML algorithms. Feature fusion plays a vital role in ML [14] and we plan to upgrade GINET’s feature fusion module to further enhance the performance. Finally, we would like to improve GINET’s generalizability of processing other battery datasets and customize GINET for other battery applications such as battery health monitoring. REFERENCES [1] J. Fleischmann, M. Hanicke, E. Horetsky et al., “Battery 2030: Resilient, sustainable, and circular,” McKinsey & Company, pp. 2–18, 2023.", "page": 5, "position": 1}
{"chunk_id": "2501.04997v1_p5_c2", "doc_id": "2501.04997v1", "text": "Feature fusion plays a vital role in ML [14] and we plan to upgrade GINET’s feature fusion module to further enhance the performance. Finally, we would like to improve GINET’s generalizability of processing other battery datasets and customize GINET for other battery applications such as battery health monitoring. REFERENCES [1] J. Fleischmann, M. Hanicke, E. Horetsky et al., “Battery 2030: Resilient, sustainable, and circular,” McKinsey & Company, pp. 2–18, 2023. [2] M. N. Yubac, J. P. Cabalan, Y. P. C. Amarga et al., “Development and evaluation of an advanced battery management system (bms) for lithium- ion batteries in renewable energy applications,” in 2024 8th International Artificial Intelligence and Data Processing Symposium (IDAP), 2024, pp. 1–7. [3] R. R. Kumar, C. Bharatiraja, K. Udhayakumar et al., “Advances in batteries, battery modeling, battery management system, battery ther- mal management, soc, soh, and charge/discharge characteristics in ev applications,” IEEE Access, vol. 11, pp. 105 761–105 809, 2023. [4] X. Chen, H. Lei, R. Xiong et al., “A novel approach to reconstruct open circuit voltage for state of charge estimation of lithium ion batteries in electric vehicles,” Applied Energy, vol. 255, p. 113758, 2019. [5] F. Mohammadi, “Lithium-ion battery state-of-charge estimation based on an improved coulomb-counting algorithm and uncertainty evaluation,” Journal of Energy Storage, vol. 48, p. 104061, 2022. [6] C. Vidal, P. Malysz, M. Naguib et al., “Estimating battery state of charge using recurrent and non-recurrent neural networks,” Journal of Energy Storage, vol. 47, p. 103660, 2022. [7] T. Lin, Y. Wang, X. Liu et al., “A survey of transformers,” AI Open, vol. 3, pp. 111–132, 2022. [8] Z. Bao, J. Nie, H. Lin et al., “Ttsnet: State-of-charge estimation of li-ion battery in electrical vehicles with temporal transformer-based sequence network,” IEEE Transactions on Vehicular Technology, vol. 73, no. 6, pp. 7838–7851, 2024. [9] X. Song, F. Yang, D. Wang et al., “Combined cnn-lstm network for state-of-charge estimation of lithium-ion batteries,” IEEE Access, vol. 7, pp. 88 894–88 902, 2019. [10] J. Chung, C. Gulcehre, K. Cho et al., “Empirical evaluation of gated recurrent neural networks on sequence modeling,” 2014. [Online]. Available: https://arxiv.org/abs/1412.3555 [11]", "page": 5, "position": 2}
