{"chunk_id": "2112.00652v2_p0_c0", "doc_id": "2112.00652v2", "text": "Springer Nature 2021 LATEX template Equivariant graph neural networks for fast electron density estimation of molecules, liquids, and solids Peter Bjørn Jørgensen1 and Arghya Bhowmik1* 1*Department of Energy Conversion and Storage, Technical University of Denmark, Anker Engelundsvej 1, Kgs. Lyngby, 2800, Denmark. *Corresponding author(s). E-mail(s): arbh@dtu.dk; Contributing authors: pbjo@dtu.dk; Abstract Electron density ρ(⃗r) is the fundamental variable in the calculation of ground state energy with density functional theory (DFT). Beyond total energy, features and changes in ρ(⃗r) distributions are often used to capture critical physicochemical phenomena in functional mate- rials. We present a machine learning framework for the prediction of ρ(⃗r). The model is based on equivariant graph neural networks and the electron density is predicted at special query point vertices that are part of the message passing graph, but only receive mes- sages. The model is tested across multiple data sets of molecules (QM9), liquid ethylene carbonate electrolyte (EC) and LixNiyMnzCo(1- y-z)O2 lithium ion battery cathodes (NMC). For QM9 molecules, the accuracy of the proposed model exceeds typical variability in ρ(⃗r) obtained from DFT done with diﬀerent exchange-correlation function- als. The accuracy on all three datasets is beyond state of the art and the computation time is orders of magnitude faster than DFT. Keywords: machine learning, charge density, deep learning 1 arXiv:2112.00652v2 [physics.comp-ph] 29 Aug 2022", "page": 0, "position": 0}
{"chunk_id": "2112.00652v2_p1_c0", "doc_id": "2112.00652v2", "text": "Springer Nature 2021 LATEX template 2 DeepDFT 1 Introduction Simulations are as critical as experiments now in materials discovery. At the atomic scale, quantum mechanics based simulations are frequently used in the computational search of novel functional materials and molecules[1]. Within the well-known cost-accuracy trade-oﬀassociated with such methods, Kohn- Sham density functional theory (DFT) is the most widely used method due to the right balance between computational cost and accuracy. The electron density ρ(⃗r) is one of the fundamental variables in the state of the art iter- ative scheme of DFT. The electron density uniquely determines the ground state properties of a system[2]. DFT is an O(n3) complexity method and thus is limited to a few hundred atoms in system size that can be simulated. Sys- tem size limits prohibit us from fully exploiting DFT for simulating critical technologically and scientiﬁcally important systems. For example, one would need large simulation cells for portraying engineering ceramics (with many types of atoms in small fractions) or mixed liquid electrolytes (with many component molecules and additives). Not just system size, in many materi- als design problems, the enormity of phase space to be explored can also be a bottleneck in using DFT. Total energy is the most commonly used output from DFT simulations. Signiﬁcant recent developments towards high accuracy machine learning potentials for molecules and condensed matter phases have been able to provide QM accuracy total energy at a much lower computational cost[3–5]. However, for functional materials, electronic structure is important as well. Electronic density distribution and its modulation due to structural and chemical modiﬁcations are descriptors for many chemical properties. For example, Bader charge analysis is frequently used to understand redox pro- cesses and related phenomena in intercalation battery cathodes[6–9]. Charge densities are critical for solar cell materials properties as well[10, 11]. Similarly, charge density redistribution is often used to understand trends in catalytic activity[12–15]. In liquid electrolytes, the analysis of gradients and other fea- tures in charge density helps us understand intermolecular interactions[16–19]. Charge density also gives us access to functional properties (through surro- gate models) that are costly to calculate", "page": 1, "position": 0}
{"chunk_id": "2112.00652v2_p1_c1", "doc_id": "2112.00652v2", "text": "cesses and related phenomena in intercalation battery cathodes[6–9]. Charge densities are critical for solar cell materials properties as well[10, 11]. Similarly, charge density redistribution is often used to understand trends in catalytic activity[12–15]. In liquid electrolytes, the analysis of gradients and other fea- tures in charge density helps us understand intermolecular interactions[16–19]. Charge density also gives us access to functional properties (through surro- gate models) that are costly to calculate directly. For example, charge density maps can give us direct access to optimum intercalation sites[20] as well as ion migration pathway and barriers[21]. We exemplify a few of the many pos- sible ways fast machine learning prediction of charge density can help us ﬁnd new better functional materials. We can explore larger phase space as well as evaluate functional properties of materials that require large simulation boxes. Electron density is inherently more information rich than total energy and therefore learning from the density could lead to machine learning models that generalize better from small datasets. For example, [22] and [23] both found that learning the electron density and then predicting the total energy gives better accuracy when extrapolating from small to large systems in comparison to direct energy prediction. In the last few years, a number of articles have been published on electron density prediction. Pioneering works by [24, 25]", "page": 1, "position": 1}
{"chunk_id": "2112.00652v2_p2_c0", "doc_id": "2112.00652v2", "text": "Springer Nature 2021 LATEX template DeepDFT 3 use a basis representation for the density and predict the basis function coeﬃ- cients using kernel ridge regression. The model’s eﬃcacy was demonstrated on molecular dynamics trajectories of small molecules, but by construction the model is not transferable to new molecules. A transferable model based on symmetry-adapted Gaussian process regression[26] (SA-GPR) was introduced later [27, 28]. The transferability is achieved by decomposing the density into atom-centered contributions and the local environment around each atom is mapped to a set of basis coeﬃcients using the SA-GPR framework. One of the downsides of kernel regression is that the computational complexity of the model grows cubically with the number of training examples and in most prac- tical problems we will need thousands of training examples to cover the system of interest. Deep neural network models are highly ﬂexible and are generally well suited for absorbing large datasets. A 3D convolutional neural network has been [29] trained with thousands of small molecules. However, by using a voxel based 3D U-Net [30] architecture the model is dependent on the image resolution and is not equivariant to rotations. Equivariance to rotation has been achieved in diﬀerent ways. The aforementioned SA-GPR model [27, 28] has symmetry built into its kernel function. For deep learning models, equivariance has been achieved by constructing a ﬁngerprint for every point in space that is invariant to rotations around the point for which the ﬁngerprint has been created, but not to rotations around other points. [31–34]. The models are local, which means that a cutoﬀdistance deﬁnes the range for which the atoms no longer inﬂuence the electron density. Message- passing neural networks [35, 36] provide a mechanism for propagating atomic interactions over longer distances in a computationally eﬃcient manner, by computing local messages that represents an atom and its environment and then propagate this information via the edges of a graph representation of the system. Message-passing neural networks are also being applied to the electron density prediction problem. In the work by [37] a message passing net- work is used as part of the algorithm,", "page": 2, "position": 0}
{"chunk_id": "2112.00652v2_p2_c1", "doc_id": "2112.00652v2", "text": "a mechanism for propagating atomic interactions over longer distances in a computationally eﬃcient manner, by computing local messages that represents an atom and its environment and then propagate this information via the edges of a graph representation of the system. Message-passing neural networks are also being applied to the electron density prediction problem. In the work by [37] a message passing net- work is used as part of the algorithm, but a new graph, representing the local neighborhood, is created for every point in space, which makes the method computationally ineﬃcient and the model was therefore only trained on a rel- atively small number of points. A more eﬃcient approach was later presented by [38][39] in which atom-centered density contributions are predicted with a message passing neural network. This approach allows the energy of the sys- tem to be calculated analytically, but the accuracy of the density predictions are inferior to our previously published framework, DeepDFT, which predicts the densities directly point by point [40]. In the previous message passing solutions [37–40] an invariant representation is used. However, in this ﬁrst generation of message passing algorithms, the models are unable to resolve angular information through the message passing [41]. Recent developments in equivariant message passing neural networks [41–46] make it possible to prop- agate directional information through the message passing steps from which", "page": 2, "position": 1}
{"chunk_id": "2112.00652v2_p3_c0", "doc_id": "2112.00652v2", "text": "Springer Nature 2021 LATEX template 4 DeepDFT angular information can be extracted. Common for the equivariant message passing models is that the hidden states of the graph nodes are now represent- ing directional vectors (in three dimensions) that rotate with the rotation of the molecule. In this work, we present the equivariant DeepDFT model, which is a machine learning model for predicting the electron density ρ(⃗r). The model is based on equivariant message passing on a graph and uses special probe nodes inserted into the graph, for which the density is computed. In contrast to the OrbNet-Equi model [46], which uses features calculated by the GFN1-xTB semiempirical electronic structure method, our method is purely data driven in the sense that the only inputs required to make a prediction are the atomic numbers and the coordinates of the atoms (including the unit cell parameters for periodic structures). We also do not induce any bias in terms of using a predeﬁned basis set for the density, i.e., the density is purely learned from data examples. Previous models have shown the ability to predict electron density on a number of diﬀerent systems, including molecular dynamics of a single molecule or slab [24, 32], diﬀerent hydrocarbon molecules [27, 33], large datasets of small organic molecules [29, 38, 46], carbon nanotubes [34], crystalline polymers and zeolites [37] and peptides [28, 39]. To showcase the universal applicability, we benchmark the equivariant DeepDFT model on three diverse datasets, (a) the QM9 dataset [47, 48] often used for benchmarking molecular machine learning models, which is a large dataset of 134k small organic molecules (b) a dataset of mixed transition metal layered oxide lithium ion battery cathode materials and (c) a dataset consisting of a molecular dynamics trajectory with ethylene carbonate molecules - a commonly used organic electrolyte. 2 Results 2.1 Invariant and equivariant message passing models We have developed and trained two models for charge density prediction with diﬀerent architectures, but both work on the principles of message passing on molecular graphs[35]. In this section we will be referring to both of the models - the invariant DeepDFT", "page": 3, "position": 0}
{"chunk_id": "2112.00652v2_p3_c1", "doc_id": "2112.00652v2", "text": "(c) a dataset consisting of a molecular dynamics trajectory with ethylene carbonate molecules - a commonly used organic electrolyte. 2 Results 2.1 Invariant and equivariant message passing models We have developed and trained two models for charge density prediction with diﬀerent architectures, but both work on the principles of message passing on molecular graphs[35]. In this section we will be referring to both of the models - the invariant DeepDFT model and the equivariant DeepDFT model. A conceptual overview of the equivariant and invariant DeepDFT models are shown in Fig. 1. The models use a 3D-embedded graph representation of the molecule or crystal structure. The graph has a vertex for each atom in the molecule or for each atom in the crystal structure unit cell. Edges are deﬁned by a constant cutoﬀdistance, i.e. we draw an edge between vertices if the distance between them is less than a certain cutoﬀdistance (chosen here to be 4 \u0006A). The edges may cross the periodic boundary as in quotient graphs [49, 50]. Special probe vertices, that only accept incoming edges, are placed at each query point where electron density prediction is to be made. Each vertex has a hidden state, which represents the atom or probe and its environment and the state is initialized", "page": 3, "position": 1}
{"chunk_id": "2112.00652v2_p4_c0", "doc_id": "2112.00652v2", "text": "Springer Nature 2021 LATEX template DeepDFT 5 based on the atom type or is zero for the probe vertices. The vertices interact by receiving messages from other vertices via the incoming edges. After each exchange of messages, the vertices update their hidden state based on the sum of incoming messages. Artiﬁcial neural networks (ANN) are used to model - (a) how the content of the messages depend on sending/receiving vertices and (b) how the hidden state updates depend on the message sum. ANNs representing these interactions can be trained using data examples. After a number of interaction steps T another neural network is used to map the hidden state of each probe vertex to the predicted density at that point in space. In the invariant version of the DeepDFT model, the edge feature is the distance between the vertices, while in the equivariant version, the distance as well as the direction of the edges are used as features. The directional features aﬀect the hidden state of the vertices. To maintain the directionality of the hidden states, the equivariant model contains two sets of hidden states, an array of vector valued features for each vertex in addition to the array of scalar valued features for each vertex. The equivariance of the hidden states is preserved by restricting the allowed operations to scaling, inner products with other equivariant features and the addition of linear combinations of other equivariant features. The details of the invariant DeepDFT model are described in our previous NeurIPS workshop paper [40] and more details of the equivariant model can be found in the methods section. 2.2 Dataset and setup To assess the models, we use three diverse datasets. The ﬁrst is the QM9 dataset [47, 48] (134k small molecules with up to nine heavy atoms (CNOF)) that is widely used for benchmarking machine learning models for molecular property prediction. Additionally, we also train and test with electron den- sity data from crystalline and liquid state materials. 1: A class of industrially important mixed transition metal layered oxide lithium ion battery (LIB) cathode materials. Conﬁgurations are generated through random", "page": 4, "position": 0}
{"chunk_id": "2112.00652v2_p4_c1", "doc_id": "2112.00652v2", "text": "The ﬁrst is the QM9 dataset [47, 48] (134k small molecules with up to nine heavy atoms (CNOF)) that is widely used for benchmarking machine learning models for molecular property prediction. Additionally, we also train and test with electron den- sity data from crystalline and liquid state materials. 1: A class of industrially important mixed transition metal layered oxide lithium ion battery (LIB) cathode materials. Conﬁgurations are generated through random crystal site occupation of transition metal ions (Ni/Mn/Co) and lithium/vacancy to rep- resent varieties of chemistry and lithiation states. 2: Liquid ethylene carbonate - the most used LIB electrolyte. 12000 disordered conﬁgurations are generated through high temperature (3000K) accelerated molecular identity preserving molecular dynamics. In all three cases, the electron densities are obtained using the VASP code [51]. See the methods section for more details on the computational setup. 2.3 Prediction accuracy In this numerical experiment we assess the average prediction accuracy of the model using the three aforementioned datasets. We split the datasets into training, validation (for model selection) and test set with uniform random splitting. The sizes of the splits are shown in table 1. To evaluate the model’s accuracy in predicting charge density for a given atomic structure, we integrate", "page": 4, "position": 1}
{"chunk_id": "2112.00652v2_p6_c0", "doc_id": "2112.00652v2", "text": "Springer Nature 2021 LATEX template DeepDFT 7 The equivariant DeepDFT achieves notably lower prediction error than the invariant model, especially for the ethylene carbonate dataset. Both methods produce roughly two orders of magnitude lower error than the superposition of atomic densities baseline . The QM9 accuracy is below that of OrbNet- Equi[46], which uses additional input information, i.e. the results of GFN1-xTB semiempirical electronic structure method are used as input features. Thus it can not be considered as a pure data driven model. As another reference, we also look at the variation in electron density between DFT simulations per- formed using diﬀerent exchange-correlation functionals in VASP. This can be seen as an appraisal of variability in DFT derived charge density. We randomly select 1000 examples of the QM9 test set and recalculate them with eight diﬀerent XC functionals in VASP (BEEF, Perdew-Burke-Ernzerhof, Perdew - Wang 91, Ceperley-Alder, Perdew-Zunger, revised Perdew-Burke-Ernzerhof, revPBE, PBEsol) [52–59]. For every grid point, we select the median across the eight calculated densities as the reference point and compute the mean absolute deviation around this point as shown in Equation (2). εmad = R ⃗r∈V 1 K PK k=1 |ρk(⃗r) −ρmedian(⃗r)| R ⃗r∈V |ρ1(⃗r)| (2) For every molecule, the deviation is numerically integrated over the simulated volume. This serves as an estimate of inherent variations in DFT derived den- sity and is analogous to the error measure Equation (1) used for the machine learning models. The DFT variation of 1000 molecules is shown along with the prediction test set (10000 molecules) errors in Fig. 2. The average DFT vari- ation across molecules is 0.60 %, which is generally higher than the DeepDFT error. However, notice that the distribution of errors from the DeepDFT model has a much longer tail than DFT variation. There is even a single test point with 11 % error (not visible on the histogram ﬁgure). This is not the case for the two other datasets and is caused by the large chemical and structural varia- tions within the QM9 dataset. Modeling the uncertainty and detecting outliers is therefore important for future models. Error", "page": 6, "position": 0}
{"chunk_id": "2112.00652v2_p6_c1", "doc_id": "2112.00652v2", "text": "distribution of errors from the DeepDFT model has a much longer tail than DFT variation. There is even a single test point with 11 % error (not visible on the histogram ﬁgure). This is not the case for the two other datasets and is caused by the large chemical and structural varia- tions within the QM9 dataset. Modeling the uncertainty and detecting outliers is therefore important for future models. Error isosurfaces for four example molecules are shown in Fig. 3. We are showing two very high error examples (Fig. 3a and Fig. 3b), an average error datapoint (Fig. 3c) and the lowest error example (Fig. 3d) in the test set. The highest error example (Fig. 3a) is a clear outlier and has very unnatural bond angles. The ammonia example (Fig. 3b) is an isolated group and is therefore not well represented in the training data. The average and lowest error points (Fig. 3c and Fig. 3d) are well represented in the dataset and especially the hydrocarbon (Fig. 3d) is easily learned. Pre- vious work focusing only on hydrocarbons could create models reaching an accuracy of 1.26 % using less than 1000 DFT densities[33]. Our models are more accurate for hydrocarbons while generalizing to other molecules as well. To understand the accuracy of the model from an energy perspective, we take all the charge densities of the test sets and replace the density values", "page": 6, "position": 1}
{"chunk_id": "2112.00652v2_p7_c0", "doc_id": "2112.00652v2", "text": "Springer Nature 2021 LATEX template 8 DeepDFT Fig. 2: Histogram of test errors on QM9 in comparison with variations in DFT computed density using diﬀerent exchange correlations functional. The markers show bins with a count of one. Table 1: Datasets and prediction errors for invariant and equivariant DeepDFT model. We also compare with OrbNet-Equi and the superposition of atomic densities as implemented in VASP. Dataset Splits Test Set Error (εmae%) Dataset Train Val. Test invDeepDFT eqDeepDFT OrbNet-Equi[46] Init VASP QM9 123835 50 10000 0.36 0.27 0.21 15 LIB Cath. 1450 50 500 0.09 0.06 7.1 Eth. Carb. 7330 50 4000 0.53 0.18 13 with those predicted by the equivariant DeepDFT model (while keeping the PAW augmentation charges ﬁxed), and run a single-point non-self-consistent energy calculation using VASP. The obtained energies are compared with the self-consistent energies and the energy errors are shown in Fig. 4. The distribution of energy errors is heavily distributed around 0 with a long nar- row tail, so we show the distribution in a log-log histogram. For the QM9 dataset there are three molecules out of 10 000 with absolute error above 1 × 10−2 eV/atom, while the mean absolute error for the remaining molecules is 8.5 × 10−5 eV/atom. For the two other datasets we observe less extreme outliers, but the tails are still present. The MAEs are 4.2 × 10−4 eV/atom and 1.2 × 10−4 eV/atom for the LIB cathode and ethylene carbonate datasets, respectively. 2.4 Learning curve In the sections above, we have looked at the average test errors for speciﬁc training and test set sizes. To better understand the eﬀectiveness of the learn- ing method and the data eﬃciency of the models, it can be very useful to look", "page": 7, "position": 0}
{"chunk_id": "2112.00652v2_p8_c0", "doc_id": "2112.00652v2", "text": "Springer Nature 2021 LATEX template DeepDFT 9 (a) C8H8O (1.1%) (b) NH3 (1.0%) (c) C3H6N2O (0.28%) (d) C8H18 (0.13%) Fig. 3: Prediction error isosurfaces ±0.001 e Bohr−3 for four example QM9 molecules from the test set with hydrogen (white), carbon (brown), oxygen (red) and nitrogen (blue). The error percentages in parenthesis denote the normalized mean absolute error εmae for each molecule. The chosen examples are two very high error molecules (a) and (b), an example with average error (c) and the molecule with the lowest error of all in the test set (d). at learning curves [60], i.e. test error as a function of the training set size plot- ted on a log-log scale. The validation and test sets are the same as above, but we randomly sample a subset of the training data to reduce the training set size. The result of this numerical experiment is shown in Fig. 5. Ideally the learning curves should follow a straight line in the log-log plot and be as steep as possible [60]. Initially all the learning curves are steep, but they ﬂatten out with an increasing number of training examples. The ﬂattening of a learning curve is usually caused by either noise in the data, by a non-unique input representation or by lack of ﬂexibility in the model. If the saturation was caused by noise we would expect both models to converge to the same error. In contrast, for all three datasets, the equivariant models outperform the invariant models. The input information presented to", "page": 8, "position": 0}
{"chunk_id": "2112.00652v2_p9_c0", "doc_id": "2112.00652v2", "text": "Springer Nature 2021 LATEX template 10 DeepDFT 10 3 10 4 10 4 10 3 10 2 10 1 100 101 102 Energy error [eV/atom] 100 101 102 103 104 count (a) QM9 10 2 10 3 10 4 10 4 10 3 10 2 10 1 Energy error [eV/atom] 100 101 102 103 count (b) LIB Cathode 10 2 10 3 10 4 10 4 10 3 10 2 Energy error [eV/atom] 100 101 102 103 104 count (c) Ethylene Carbonate Fig. 4: Distribution of energy errors obtained from running non-self-consistent single point energy calculations with VASP using the predicted DeepDFT charge densities. The distribution of normalized (by number of atoms) energy errors are shown for three diﬀerent datasets QM9 (a), LIB Cathode (b) and Ethylene Carbonate (c). The hisograms use logarithmic bins and count scale to clearly show both the heavy concentration around 0 eV/atoms and the long, narrow tail of errors. the equivariant models is the relative positions of atoms and their atomic numbers, which is (given a large enough cutoﬀ) enough to distinguish between all the inputs. This indicates that the model lacks the ﬂexibility to capture all the details and inter-dependencies modeled by density functional theory and this is not surprising, given the simpler architecture of the deep learning models and the approximate nature of the learned functions. The training curves show (see Supplementary Figure 1 and Supplementary Figure 2 ) that with 15 000 training examples or more the model is not able to signiﬁcantly overﬁt the QM9 dataset, but in all other cases the root mean squared error (RMSE) is lower on the training set than on the validation set. During development we have brieﬂy tried to double the number of interaction layers in the equivariant model and increased the cutoﬀradius to 5 \u0006A, but we did not see an improvement in the accuracy of the model. However, it might be possible to improve the model with a more systematic hyperparameter search or by introducing higher order equivariant internal representations as used in recently developed interatomic potentials, for example NequIP [61]. Improved models", "page": 9, "position": 0}
{"chunk_id": "2112.00652v2_p10_c0", "doc_id": "2112.00652v2", "text": "Springer Nature 2021 LATEX template DeepDFT 11 102 103 104 105 Training size 10 1 100 Average test error [%] QM9 NMC EC Fig. 5: Learning curves for invariant (dashed lines) and equivariant (solid lines) models for three diﬀerent datasets. The plot shows the average normalized test errors Equation (1) for the QM9 (blue) LIB Cathode (orange) and Ethylene Carbonate (green) datasets as functions of the number of training examples, which are randomly sampled subsets of the full training sets. 2.5 Intermolecular interactions To visualize the diﬀerence in prediction accuracy between the two models trained on the liquid ethylene carbonate structures, in Fig. 6 we show an error isosurface for one of the test set examples. We see that most of the error is located around the oxygen atoms (red atoms). To investigate this quan- titatively we partition the electron density into volumes around each atom according to Bader partitioning[62]. For the 4000 test examples this leads to 29%, 16%, 55% of the total volume and 12%, 20%, 68% of the total electron charge to be assigned to H, C, O respectively. To understand how diﬀerent ele- ments contribute to the overall error, we calculate εmae following Equation (1); but for each atom type we only integrate over the Bader volumes associated with atoms of that type. Instead of normalizing with respect to the target den- sity for each atom type in Equation (1), we also normalize with respect to the total error and calculate the total error share. The error decomposition for the invariant and equivariant models are shown in table 2. As the error isosurface ﬁgure also showed, the majority of the error is assigned to the oxygen atoms (66% for the invariant model and 59% for the equivariant model), but most of the target electron density is also found within the oxygen Bader volumes (68%). We also notice that out of the three elements", "page": 10, "position": 0}
{"chunk_id": "2112.00652v2_p11_c0", "doc_id": "2112.00652v2", "text": "Springer Nature 2021 LATEX template 12 DeepDFT Fig. 6: Prediction error isosurfaces of ±0.003 e Bohr−3 with model based on invariant representation to the left and model based on equivariant representa- tion to the right. The ethylene carbonate molecule contains hydrogen (white), carbon (brown) and oxygen (red). Table 2: Prediction errors of liquid ethylene carbonate data test set decom- posed into Bader volumes for each atom type. Model Element εmae Total Error share H 0.71% 17% Invariant C 0.41% 16% O 0.49% 66% H 0.32% 21% Equivariant C 0.18% 20% O 0.16% 59% it is the oxygen volume that beneﬁts the most from using the equivariant model. 2.6 Runtime and Scalability To demonstrate the scalability of the model we measure the runtime for calcu- lating the electron density of systems of increasing sizes. We use a single cell of 12 atoms Li3Co2NiO6 with periodic boundary conditions and repeat the unit cell to show how model run time and DFT run time scales with system size. The result of the scalability test is plotted in Fig. 7. As expected we observe a linear trend for DeepDFT and it is therefore much faster than DFT for large systems, because of the cubic complexity of DFT. However, DeepDFT is also an order of magnitude faster even for small systems. Notice that DeepDFT is only running on a single GPU core and can be optimized to utilise more GPU cores in parallel for actual deployment towards high throughput tasks. The", "page": 11, "position": 0}
{"chunk_id": "2112.00652v2_p12_c0", "doc_id": "2112.00652v2", "text": "Springer Nature 2021 LATEX template DeepDFT 13 101 102 103 104 Number of atoms 100 101 102 103 Run time [s] DFT DeepDFT Fig. 7: Computation time for VASP running on 2*20 core Intel Skylake Xeon CPU vs DeepDFT running on a single RTX 3090 GPU. The dashed lines show the expected asymptotic behaviours ax3 and ax respectively. model is implemented in PyTorch for research purposes and even though it is orders of magnitude faster than density functional theory for large systems, it can still be made to run faster, e.g. utilising several GPUs in parallel when making predictions or by simplifying the readout network, because this part of the network needs to be run for point in the simulation grid and thus are completely parallelizable with no communication overhead. 2.7 Charge transfer in NMC Cathode Charge transfer in intercalation cathodes is not only a key descriptor for the ion intercalation process [20] and related energetics, but can also be used as a tool towards capturing degradation processes like oxygen evolution [63]. Thus understanding the electrochemical properties of cathode materials and com- putational screening for materials need access to charge density at various lithiated states. In this section we investigate the applicability of the model to accurately track charge transfer in NMC cathode materials. To test the accu- racy of the trained DeepDFT model, we randomly sample 30 structures from the NMC dataset (that are not part of the training dataset), remove one Li atom and relax the structure with DFT. The DeepDFT model is then used to predict the electron density of the initial structure and of the one with one Li removed. If DeepDFT can capture the electron transfer redox process with good accuracy, we can use such models for screening optimal NMC com- positions. We do note that this numerical experiment is only meaningful for assessing the ability of the model to track charge transfer. In a real computa- tional screening setting the structure relaxation would need to also be replaced by a machine learning model. To assign charge to each atom we use the Bader charge", "page": 12, "position": 0}
{"chunk_id": "2112.00652v2_p13_c0", "doc_id": "2112.00652v2", "text": "Springer Nature 2021 LATEX template 14 DeepDFT for all the 30 systems is shown as a histogram in Fig. 8. Notice that we have two clusters of charge diﬀerences in the dataset. The largest cluster is atoms that are not or very little aﬀected by the removal of the Li atom. The smaller cluster is the one that is inﬂuenced by the Li removal to a larger degree. The prediction error is small for both clusters as shown in Fig. 8. This reinforces the capability of DeepDFT as a practical ML model that can be deployed for inexpensive large phase space exploration for high performance materials. Instead of looking at individual errors we can also compute the total error of each structure. We calculate the sum of absolute errors in the charge diﬀerence across the unit cell and average across the 30 structures of the test set: εtotal = 1 30 30 X i=1 |Qi| X n=1 ∥∆qi n −∆ˆqi n∥ (3) where i is the index for the 30 systems in the test set, ∆qi n is the change in Bader charge of the nth atom of that system and |Qi| is the number of atoms in the ith system. The average total error is 0.060 e, which means that on average 6.0 % of the electron charge is distributed incorrectly. However, a large proportion of that error is due to ﬂuctuations in charges far away from the removed Li. If we only consider the near atoms in the inner sum of Equation (3), the error decreases to 1.0 %. 2 4 6 Distance from Li [Å] 0.00 0.05 0.10 0.15 Bader difference [e] Li O Mn Co Ni 0 100 Count 0 250 Count 0.010 0.005 0.000 0.005 0.010 Prediction error in Bader difference [e] 0 25 50 75 100 125 150 175 200 Count Near Far Fig. 8: Analysis of Bader charge diﬀerence of remaining atoms when removing a Li atom from NMC cathode. The scatter plot to the left shows the change in Bader charge for all atoms in the systems as a function of the", "page": 13, "position": 0}
{"chunk_id": "2112.00652v2_p14_c0", "doc_id": "2112.00652v2", "text": "Springer Nature 2021 LATEX template DeepDFT 15 3 Discussion In this work we have presented the equivariant DeepDFT model which achieves at par or beyond the state of the art prediction accuracy on three widely dif- ferent datasets including solid, liquid, and gas phases. Implementation of the equivariant framework (improvement on the invariant one [40]) greatly helped in achieving high accuracy, especially in liquid state simulations as it helped learning better representations of inter- and intramolecular structure varia- tions. Although the training is done on discrete grid points, the learnt function is fully diﬀerentiable and thus can be used for further mathematical deriva- tions, such as derivative-based visualization of interactions using Interaction Region Indicator (IRI) [65] or Density Overlap Regions Indicator (DORI) [66]. Additionally, the inference mechanism itself is parallelizable and can be dis- tributed to a larger number of GPUs, which makes electron density simulation of millions of atoms feasible. Surrogate models for energy and forces have helped perform high accuracy molecular simulations at long length and time scale or conduct high through- put screening at an unprecedented speed in the last few years. For functional materials discovery (e.g. battery electrodes, catalysts etc.) combining energy models with DeepDFT will let us model and improve properties where electron transfer redox reactions and charge density are critical for better functional- ity. Because of the low runtime and linear scaling with system size DeepDFT will let us explore far larger materials phase space than it is currently possible with DFT derived charge densities. We foresee high throughput screening and large scale simulations pertaining to redox reactions and charge transfer properties will be tackled with DeepDFT in the near future. The error distribution for structures not well represented in the training data creates a long tail with high errors (as seen for a few unique molecules in QM9 dataset). For using DeepDFT in high throughput studies and for building trained models for new classes of materials in a data eﬃcient manner with active learning, it would be necessary to also model the uncertainty of the predictions, e.g. by using an ensemble of models.", "page": 14, "position": 0}
{"chunk_id": "2112.00652v2_p14_c1", "doc_id": "2112.00652v2", "text": "for structures not well represented in the training data creates a long tail with high errors (as seen for a few unique molecules in QM9 dataset). For using DeepDFT in high throughput studies and for building trained models for new classes of materials in a data eﬃcient manner with active learning, it would be necessary to also model the uncertainty of the predictions, e.g. by using an ensemble of models. In future it will also be beneﬁcial to benchmark the model on other density derived properties such as the total energy, as utilizing density as the underlying variable can be more data eﬃcient strategy than predicting the properties directly. The DeepDFT code is made available for the community and we are looking forward to seeing applications of the model in the simulation of materials and molecules as well as adaption into other models expanding on our codebase. 4 Methods 4.1 Equivariant Neural Message Passing Network In this section we describe more formally and in more detail how neural mes- sage passing is used to model the electron density around atoms. The DeepDFT", "page": 14, "position": 1}
{"chunk_id": "2112.00652v2_p15_c0", "doc_id": "2112.00652v2", "text": "Springer Nature 2021 LATEX template 16 DeepDFT density model is framed in the neural message passing framework devised by [35]. A simple example graph with four atoms and three query points is illus- trated in Fig. 9. Each vertex has a hidden state that is updated in a number of interaction steps. Fig. 9: Neural message passing with probe nodes. Neural network computed messages are exchanged between atom vertices in several steps while the probe nodes only receive messages. The contents of the messages depend on the hidden state of the vertices and the startes are updated after each message- passing step. The previously introduced DeepDFT model [40] uses an atom-to-atom interaction architecture very similar to the SchNet model [36]. However, in this type of model the hidden states of the vertices are scalar arrays, which con- tain no explicit information about directionality and are invariant to rotations. This for example, means that a change in angles can not be resolved through message passing steps [41]. This shortcoming has already been addressed by graph neural networks using spherical harmonics as the irreducible represen- tation for the group of rotations in three dimensions [44, 45, 61]. In these cases the hidden states become higher order tensors and they rotate (equivariantly) with the molecule. A special case of equivariant neural networks, for which the equivariant states are Cartesian tensors, was recently introduced by [41]. In the so called polarizable atom interaction neural network (PaiNN) model the hidden vertex state contains a scalar array state as well as a vectorial state. Equivariance of the vectorial state is conserved by restricting the vectorial states to interact only via cross products, inner products and scaling. The new version of DeepDFT uses a variant of the PaiNN architecture as backend and the architectural details are given in this section. See the article introducing PaINN [41] for more explanation on the method itself. The input to the model are the atomic numbers {z1, · · · , zN} ∈N, the xyz-coordinate of each atom {⃗r1, · · · ,⃗rN} ∈R3 and of each probe point {⃗p1, ·", "page": 15, "position": 0}
{"chunk_id": "2112.00652v2_p16_c0", "doc_id": "2112.00652v2", "text": "Springer Nature 2021 LATEX template DeepDFT 17 From this information a directed graph is constructed with a vertex for each atom and another vertex type for each probe point. The edges of the graph are drawn between atoms when they are within the cutoﬀdistance (4 \u0006A) and incoming edges to the probe points are drawn when they are within the cutoﬀdistance of an atom. The atom scalar nodes are initialised with a learned embedding for each atom type s0 i = azi ∈RF ×1 and the vectorial state is initialised to zeros ⃗v0 i = ⃗0 ∈RF ×3. The update in scalar state is given by a sum over messages from neighboring atoms ∆sm i = X j∈N(i) φs(sj) ◦Ws(∥⃗rij∥) · fcut(∥⃗rij∥) (4) where φs is a 2-layer neural network with hidden layer and output layer size F, ◦is element-wise vector multiplication and Wm s (∥⃗rij∥) is a continuous ﬁlter function. The feature-wise ﬁlter function is implemented as F linear combinations of the distance expanded in sinc-like radial basis function [67] sin \u0010 nπ rcut ∥⃗rij∥ \u0011 / ∥⃗rij∥with 1 ≤n ≤20. The cutoﬀfunction fcut(∥⃗rij∥) = 0.5(cos(π ∥⃗rij∥/rcut)+1) for ∥⃗rij∥< rcut and 0 otherwise as proposed by [68]. The cutoﬀfunction ensures a smooth transition when neighboring atoms enter the cutoﬀregion. The update in vectorial state is given by: ∆⃗vm i = X j∈N(i) ⃗vj ◦φvv(sj) ◦Wvv (∥⃗rij∥) · fcut(∥⃗rij∥) + X j∈N(i) φvs(sj) ◦Wvs (∥⃗rij∥) ⃗rij ∥⃗rij∥· fcut(∥⃗rij∥) (5) The ﬁrst sum is the convolution of scaled equivariant features ⃗vj ◦φvv(sj) with an invariant (only distance-dependent) ﬁlter function. This is the only opera- tion in the architecture where equivariant features are propagated through the network. This allows directional information obtained through previous mes- sage passing interactions to propagate through the network. The second term in Equation (5) is the only operation in the architecture where new directional information is added to the hidden state. Unit vectors corresponding to edges between vertices are scaled by φvs(sj) and by the invariant ﬁlter function Wvs (∥⃗rij∥) fcut(∥⃗rij∥) and added to the vectorial node state. For more expressiveness another two update equations are introduced,", "page": 16, "position": 0}
{"chunk_id": "2112.00652v2_p17_c0", "doc_id": "2112.00652v2", "text": "Springer Nature 2021 LATEX template 18 DeepDFT which is a scaling of a linear combination of equivariant vector features. The message passing algorithm works by computing Equation (4) and Equation (5) in parallel for all the node vectors and add the computed values to the current scalar and vectorial states respectively. Then the nodes are updated by running Equation (6) and Equation (7) in parallel and update the nodes atomwise. The message passing and update equations are repeated in several layers with diﬀerent neural network weights at each layer. In the PaiNN method and in this work we use 3 layers of message passing and update layers. The hidden state of the special probe vertices are initialised with zeros (the scalar features are zero and the equivariant features are the zero vector). They use the same message passing and update equations as for the atom vertices above, but the neural network weights are not shared between the two. Furthermore, instead of using the residuals in Equation (4) and Equation (5) directly, we introduce a gating network that determines which features of the message sum to include and which part of the features to ignore. snew i = Gs(∆sm i ) ◦si + (1 −Gs(∆sm i )) ◦∆sm i (8) ⃗vnew i = Gv(∆sm i ) ◦⃗vi + (1 −Gv(∆sm i )) ◦∆⃗vm i (9) The gating neural networks are two-layer neural networks with SiLU activation function for the hidden units and a sigmoid output activation function. This allows the network to ignore parts of the incoming messages dependent on the total sum of messages. After the ﬁnal interaction steps the ﬁnal state of each probe vertex is mapped to a single scalar for each probe state si. As in the original PaiNN model we also use a two layer neural network with SiLU activation function on the ﬁrst layer and a linear activation function on the output layer. In principle the vectorial representation also enables prediction of vectorial properties at each probe vertex or higher order tensors constructed from a rank-1 tensor decomposition, as described in the original PaiNN article", "page": 17, "position": 0}
{"chunk_id": "2112.00652v2_p17_c1", "doc_id": "2112.00652v2", "text": "single scalar for each probe state si. As in the original PaiNN model we also use a two layer neural network with SiLU activation function on the ﬁrst layer and a linear activation function on the output layer. In principle the vectorial representation also enables prediction of vectorial properties at each probe vertex or higher order tensors constructed from a rank-1 tensor decomposition, as described in the original PaiNN article [41]. 4.2 Model parameters and training/validation setup In all experiments we use a feature size F = 128 and cutoﬀdistance rcut = 4 \u0006A. When using the invariant message passing model we set T = 6 and use T = 3 for the equivariant message passing model. The invariant model then has 2.1 × 106 parameters and the equivariant model 1.5 × 106 parameters. Because of the large memory requirement for the electron density (CHGCAR) ﬁles, we use a rotating pool of 20 atomic conﬁgurations during training. New conﬁgu- rations are continuously loaded from the full dataset on disk into the rotating pool. In each training step we sample two atomic structures and for each struc- ture 1000 probe points are uniformly sampled from the VASP electron density grid . The cost function is the mean squared error of the probe points. The Adam optimizer is used with initial learning rate 10−4 and the learning rate is exponentially decayed during training, i.e. the learning rate is 0.96s·10−5 at gradient step number s. The validation set is used for early stopping. The cost", "page": 17, "position": 1}
{"chunk_id": "2112.00652v2_p18_c0", "doc_id": "2112.00652v2", "text": "Springer Nature 2021 LATEX template DeepDFT 19 -inf 10 510 410 310 210 1 100 101 102 density [eÅ 3] 0 5 10 15 20 normalized count [%] (a) QM9 -inf 10 510 410 310 210 1 100 101 102 density [eÅ 3] 0 20 40 60 normalized count [%] (b) NMC -inf 10 510 410 310 210 1 100 101 102 density [eÅ 3] 0 10 20 30 40 normalized count [%] (c) Ethylene carbonate Fig. 10: Normalized histograms with for the data examples of the three electron density datasets. QM9 (??) LIB Cathode (b) and Ethylene Car- bonate (??). The histograms use logarithmic bins and show the number of data examples that fall into each bin as a percentage of the total number of examples. on the validation set is computed every 5000 gradient steps and the ﬁnal model is the one with the lowest error on the validation set. To reduce the variance and improve computational eﬃciency the probes of the validation set are kept ﬁxed during the training and we use 5000 probes for each conﬁguration. 4.3 Datasets For all datasets the electron densities are calculated with VASP which is a projector-augmented wave based implementation of DFT. 400 eV is used for the wavefunction cutoﬀand a Gaussian smearing of 0.1 eV is used for the elec- tronic states. The ﬁrst Brillouin zone is sampled only at the zone center for QM9 molecules, with a 3x3x1 Monkhorst–Pack k-point mesh for NMC data and 2x2x2 for liquid ethylene carbonate simulations. Perdew-Burke-Ernzerhof (PBE) exchange-correlation functional is used for all datasets. However to understand the eﬀect of exchange correlation-functional on the variability of DFT calculated charge density we did DFT simulations with eight diﬀerent functionals for small subset of QM9 molecules. Model training was done with PBE densities. The electron densities are output on a grid and the grid spac- ing is approximately 0.1 \u0006A in all three datasets. The average number of grid points per conﬁguration is 700k, 400k and 1M for the QM9, NMC and ethy- lene carbonate datasets, respectively. The distribution of the data points for each", "page": 18, "position": 0}
{"chunk_id": "2112.00652v2_p19_c0", "doc_id": "2112.00652v2", "text": "Springer Nature 2021 LATEX template 20 DeepDFT 4.3.1 QM9 Dataset The geometries for the QM9 dataset [47, 48] are obtained from Figshare repos- itory as deposited by [69]. The VASP package only supports periodic boundary conditions so we use simulation cells with vacuum around molecules such that there is a gap of at least 4 \u0006A or more. As pointed out by a reviewer, the gap is not large enough to avoid all interactions between adjacent molecules. To quantify the interaction we have doubled the size of the unit cell of 1000 test set molecules and calculated the electron density at the same grid points as in the original dataset. The average εmae (Equation (1)) between the recalculated and original densities is 0.18%. 4.3.2 NMC Dataset This dataset contains charge densities for NMC 2x2x1 supercell (12 transition metal atoms and 12 Li/vacancy site) with varying levels of Li content. For each structure we ﬁrst randomly sample the number of Mn, Ni and Co atoms given that the total number of transition metal atoms is 12 and then randomly assign them to the transition metal positions of the lattice. Similarly the number of vacancies is uniformly sampled between 0 and 12 and vacancies are assigned to the Li site. The generated conﬁgurations are then relaxed in two steps: First we relax the atom positions with ﬁxed cell parameters and then we allow both positions and cell parameters to relax. We keep only the electron density (CHGCAR) ﬁle after the last cell relaxation step. The atoms are relaxed until forces on each atom are lower than 0.01 eV/\u0006A. 4.3.3 Ethylene carbonate Molecular Dynamics Trajectory This dataset consists of charge densities of individual snapshots from a molec- ular dynamics trajectory. We insert 8 ethylene carbonate molecules in the simulation box. To quickly explore a large part of the conﬁgurational space we put Hookean constraints on the molecular bonds(to maintain molecular iden- tity such that molecules are not torn apart at such high temperature) and run Langevin molecular dynamics with thermostat temperature of 3000 K. The simulation was run for 12380 steps of 0.5", "page": 19, "position": 0}
{"chunk_id": "2112.00652v2_p20_c0", "doc_id": "2112.00652v2", "text": "Springer Nature 2021 LATEX template DeepDFT 21 7 Acknowledgments Peter Bjørn Jørgensen and Arghya Bhowmik acknowledges ﬁnancial support from VILLUM FONDEN by a research grant (00023105) for the DeepDFT project. 8 Author Contributions Arghya Bhowmik (A.B.) has designed and outlined the project and its research goal. Peter Bjørn Jørgensen (P.B.J.) and A.B. have designed and developed the model and method. P.B.J has carried out the software engineering, includ- ing implementation of model and training framework, implementation of data workﬂows for the datasets, and analysis of data and results. A.B. has super- vised the research and designed the numerical experiments. A.B. and P.B.J. have both written the manuscript through mutual discussion. 9 Competing interests The Authors declare no Competing Financial or Non-Financial Interests References [1] Marzari, N., Ferretti, A., Wolverton, C.: Electronic-structure methods for materials design. Nat. Mater. 20(6), 736–749 (2021) [2] Payne, M.C., Teter, M.P., Allan, D.C., Arias, T., Joannopoulos, a.J.: Iterative minimization techniques for ab initio total-energy calculations: molecular dynamics and conjugate gradients. Rev. Mod. Phys. 64(4), 1045 (1992) [3] Unke, O.T., et al.: Machine learning force ﬁelds. Chem. Rev. 121(16), 10142–10186 (2021) [4] Deringer, V.L., Bart´ok, A.P., Bernstein, N., Wilkins, D.M., Ceriotti, M., Cs´anyi, G.: Gaussian process regression for materials and molecules. Chem. Rev. 121(16), 10073–10141 (2021) [5] Behler, J.: Four generations of high-dimensional neural network poten- tials. Chem. Rev. 121(16), 10037–10072 (2021) [6] Kondrakov, A.O., et al.: Charge-transfer-induced lattice collapse in ni-rich ncm cathode materials during delithiation. J. Phys. Chem. C 121(44), 24381–24388 (2017) [7] Dixit, M., Markovsky, B., Schipper, F., Aurbach, D., Major, D.T.: Origin of structural degradation during cycling and low thermal stability of ni- rich layered transition metal-based electrode materials. J. Phys. Chem. C", "page": 20, "position": 0}
{"chunk_id": "2112.00652v2_p21_c0", "doc_id": "2112.00652v2", "text": "Springer Nature 2021 LATEX template 22 DeepDFT 121(41), 22628–22636 (2017) [8] Varanasi, A.K., Bhowmik, A., Sarkar, T., Waghmare, U.V., Bharadwaj, M.D.: Tuning electrochemical potential of licoo 2 with cation substitution: ﬁrst-principles predictions and electronic origin. Ionics 20(3), 315–321 (2014) [9] Kuo, L.-Y., Guillon, O., Kaghazchi, P.: On the origin of non-monotonic variation of the lattice parameters of lini 1/3 co 1/3 mn 1/3 o 2 with lithiation/delithiation: a ﬁrst-principles study. J. Mater. Chem. A 8(27), 13832–13841 (2020) [10] Chang, J., Jiang, L., Wang, G., Zhao, W., Huang, Y., Chen, H.: Lead- free perovskite compounds cssn1-xgexi3-ybry explored for superior visible- light absorption. Phys. Chem. Chem. Phys. 23, 14449–14456 (2021) [11] Barragan-Yani, D., Albe, K.: Atomic and electronic structure of perfect dislocations in the solar absorber materials cuinse 2 and cugase 2 studied by ﬁrst-principles calculations. Phys. Rev. B 95(11), 115203 (2017) [12] Castellani, N.J., Branda, M.M., Neyman, K.M., Illas, F.: Density func- tional theory study of the adsorption of au atom on cerium oxide: eﬀect of low-coordinated surface sites. J. Phys. Chem. C 113(12), 4948–4954 (2009) [13] Palmer, C., et al.: Methane pyrolysis with a molten cu–bi alloy catalyst. ACS Catal. 9(9), 8337–8345 (2019) [14] Zaﬀran, J., Toroker, M.C.: Metal–oxygen bond ionicity as an eﬃ- cient descriptor for doped niooh photocatalytic activity. ChemPhysChem 17(11), 1630–1636 (2016) [15] Vasileﬀ, A., et al.: Selectivity control for electrochemical co2 reduction by charge redistribution on the surface of copper alloys. ACS Catal. 9(10), 9411–9417 (2019) [16] Cao, B., Du, J., Cao, Z., Sun, H., Sun, X., Fu, H.: Reversibility of imido- based ionic liquids: a theoretical and experimental study. RSC Adv. 7(19), 11259–11270 (2017) [17] Mangiatordi, G.F., Hermet, J., Adamo, C.: Modeling proton transfer in imidazole-like dimers: A density functional theory study. J. Phys. Chem. A 115(12), 2627–2634 (2011) [18] del Olmo, L., Morera-Boado, C., L´opez, R., de la Vega, J.M.G.: Electron density analysis of 1-butyl-3-methylimidazolium chloride ionic liquid. J. Mol. Model. 20(6), 1–10 (2014)", "page": 21, "position": 0}
{"chunk_id": "2112.00652v2_p22_c0", "doc_id": "2112.00652v2", "text": "Springer Nature 2021 LATEX template DeepDFT 23 [19] Armakovi´c, S., Armakovi´c, S.J., Vraneˇs, M., Tot, A., Gadˇzuri´c, S.: Dft study of 1-butyl-3-methylimidazolium salicylate: a third-generation ionic liquid. J. Mol. Model. 21(9), 1–10 (2015) [20] Shen, J.-X., Horton, M., Persson, K.A.: A charge-density-based general cation insertion algorithm for generating new li-ion cathode materials. npj Comput. Mater. 6(1), 1–7 (2020) [21] Kahle, L., Marcolongo, A., Marzari, N.: High-throughput computational screening for solid-state li-ion conductors. Energy Environ. Sci. 13(3), 928–948 (2020) [22] Tsubaki, M., Mizoguchi, T.: Quantum Deep Field: Data-Driven Wave Function, Electron Density Generation, and Atomization Energy Predic- tion and Extrapolation with Machine Learning. Phys. Rev. Lett. 125(20), 206401 (2020) [23] Lewis, A.M., Grisaﬁ, A., Ceriotti, M., Rossi, M.: Learning electron densities in the condensed phase. Journal of Chemical Theory and Computation 17(11), 7203–7214 (2021) [24] Brockherde, F., Vogt, L., Li, L., Tuckerman, M.E., Burke, K., M¨uller, K.-R.: Bypassing the Kohn-Sham equations with machine learning. Nat. Commun. 8(1), 872 (2017) [25] Bogojeski, M., et al.: Eﬃcient prediction of 3D electron densities using machine learning. arXiv:1811.06255 (2018) [26] Grisaﬁ, A., Wilkins, D., Cs´anyi, G., Ceriotti, M.: Symmetry-adapted machine-learning for tensorial properties of atomistic systems. Phys. Rev. Lett. 120, 036002 (2018) [27] Grisaﬁ, A., Fabrizio, A., Meyer, B., Wilkins, D.M., Corminboeuf, C., Ceri- otti, M.: Transferable Machine-Learning Model of the Electron Density. ACS Cent. Sci. 5(1), 57–64 (2019) [28] Fabrizio, A., Grisaﬁ, A., Meyer, B., Ceriotti, M., Corminboeuf, C.: Electron density learning of non-covalent systems. Chem. Sci. 10(41), 9424–9432 (2019) [29] Sinitskiy, A.V., Pande, V.S.: Deep Neural Network Computes Electron Densities and Energies of a Large Set of Organic Molecules Faster than Density Functional Theory (DFT). arXiv:1809.02723 (2018) [30] Ronneberger, O., P.Fischer, Brox, T.: U-net: Convolutional networks for biomedical image segmentation. In: Navab, N., Hornegger, J., Wells,", "page": 22, "position": 0}
{"chunk_id": "2112.00652v2_p23_c0", "doc_id": "2112.00652v2", "text": "Springer Nature 2021 LATEX template 24 DeepDFT W.M., Frangi, A.F. (eds.) Medical Image Computing and Computer- Assisted Intervention (MICCAI). Lecture Notes in Computer Science, vol. 9351, pp. 234–241. Springer, Cham (2015) [31] Zepeda-N´u˜nez, L., Chen, Y., Zhang, J., Jia, W., Zhang, L., Lin, L.: Deep density: Circumventing the kohn-sham equations via symmetry preserving neural networks. Journal of Computational Physics 443, 110523 (2021) [32] Chandrasekaran, A., et al.: Solving the electronic structure problem with machine learning. npj Comput. Mater. 5(1), 1–7 (2019) [33] Kamal, D., Chandrasekaran, A., Batra, R., Ramprasad, R.: A charge density prediction model for hydrocarbons using deep neural networks. Machine Learning: Science and Technology 1(2), 025003 (2020) [34] Alred, J.M., Bets, K.V., Xie, Y., Yakobson, B.I.: Machine learning electron density in sulfur crosslinked carbon nanotubes. Compos. Sci. Technol. 166, 3–9 (2018) [35] Gilmer, J., Schoenholz, S.S., Riley, P.F., Vinyals, O., Dahl, G.E.: Neural message passing for quantum chemistry. In: International Conference on Machine Learning, pp. 1263–1272 (2017) [36] Sch¨utt, K.T., Sauceda, H.E., Kindermans, P.-J., Tkatchenko, A., M¨uller, K.-R.: SchNet - A deep learning architecture for molecules and materials. J. Chem. Phys. 148(24), 241722 (2018) [37] Gong, S., et al.: Predicting charge density distribution of materials using a local-environment-based graph convolutional network. Phys. Rev. B 100(18), 184103 (2019) [38] Cuevas-Zuvir´ıa, B., Pacios, L.F.: Analytical Model of Electron Density and Its Machine Learning Inference. J. Chem. Inf. Model. 60(8), 3831– 3842 (2020) [39] Cuevas-Zuvir´ıa, B., Pacios, L.F.: Machine Learning of Analytical Elec- tron Density in Large Molecules Through Message-Passing. J. Chem. Inf. Model. 61(6), 2658–2666 (2021) [40] Jørgensen, P.B., Bhowmik, A.: DeepDFT: Neural Message Passing Net- work for Accurate Charge Density Prediction. Preprint at https://arxiv. org/abs/2011.03346 (2020) [41] Sch¨utt, K., Unke, O., Gastegger, M.: Equivariant message passing for the prediction of tensorial properties and molecular spectra. In: Meila, M., Zhang, T. (eds.) Proceedings of the 38th International Conference on Machine Learning. Proceedings of Machine Learning Research, vol. 139,", "page": 23, "position": 0}
{"chunk_id": "2112.00652v2_p24_c0", "doc_id": "2112.00652v2", "text": "Springer Nature 2021 LATEX template DeepDFT 25 pp. 9377–9388. PMLR, Virtual (2021) [42] Cohen, T.S., Cohen, T.S., Nl, U.: Group equivariant convolutional net- works. In: 33rd International Conference on Machine Learning, p. 10 (2016) [43] Kondor, R., Lin, Z., Trivedi, S.: Clebsch-gordan nets: A fully fourier space spherical convolutional neural network. In: Advances in Neural Information Processing Systems, vol. 31 (2018) [44] Thomas, N., et al.: Tensor ﬁeld networks: Rotation- and translation- equivariant neural networks for 3D point clouds. Preprint at https://arxiv. org/abs/1802.08219 (2018) [45] Anderson, B., Hy, T.S., Kondor, R.: Cormorant: Covariant molecular neu- ral networks. In: Advances in Neural Information Processing Systems, vol. 32 (2019) [46] Qiao, Z., Christensen, A.S., Welborn, M., Manby, F.R., Anandkumar, A., Miller, T.F.: Informing geometric deep learning with electronic inter- actions to accelerate quantum chemistry. Proceedings of the National Academy of Sciences 119(31), 2205221119 (2022). https://doi.org/10. 1073/pnas.2205221119 [47] Ruddigkeit, L., van Deursen, R., Blum, L.C., Reymond, J.-L.: Enumer- ation of 166 billion organic small molecules in the chemical universe database GDB-17. J. Chem. Inf. Model. 52(11), 2864–2875 (2012) [48] Ramakrishnan, R., Dral, P.O., Rupp, M., von Lilienfeld, O.A.: Quantum chemistry structures and properties of 134 kilo molecules. Sci. Data 1, 140022 (2014) [49] Chung, S.J., Hahn, T., Klee, W.E.: Nomenclature and generation of three-periodic nets: The vector method. Acta Crystallogr. A 40(1), 42–50 (1984) [50] Klee, W.E.: Crystallographic nets and their quotient graphs. Cryst. Res. Technol. 39(11), 959–968 (2004) [51] Hafner, J.: Ab-initio simulations of materials using vasp: Density- functional theory and beyond. J. Comput. Chem. 29(13), 2044–2078 (2008) [52] Wellendorﬀ, J., et al.: Density functionals for surface science: Exchange- correlation model development with bayesian error estimation. Phys. Rev. B 85(23), 235149 (2012)", "page": 24, "position": 0}
{"chunk_id": "2112.00652v2_p25_c0", "doc_id": "2112.00652v2", "text": "Springer Nature 2021 LATEX template 26 DeepDFT [53] Perdew, J.P., Burke, K., Ernzerhof, M.: Generalized gradient approxima- tion made simple. Phys. Rev. Lett. 77(18), 3865 (1996) [54] Perdew, J.P., Wang, Y.: Accurate and simple analytic representation of the electron-gas correlation energy. Phys. Rev. B 45(23), 13244 (1992) [55] Ceperley, D.M., Alder, B.J.: Ground state of the electron gas by a stochastic method. Phys. Rev. Lett. 45(7), 566 (1980) [56] Perdew, J.P., Zunger, A.: Self-interaction correction to density-functional approximations for many-electron systems. Phys. Rev. B 23(10), 5048 (1981) [57] Hammer, B., Hansen, L.B., Nørskov, J.K.: Improved adsorption energet- ics within density-functional theory using revised perdew-burke-ernzerhof functionals. Phys. Rev. B 59(11), 7413 (1999) [58] Zhang, Y., Yang, W.: Comment on “generalized gradient approximation made simple”. Phys. Rev. Lett. 80(4), 890 (1998) [59] Csonka, G.I., et al.: Assessing the performance of recent density function- als for bulk solids. Phys. Rev. B 79(15), 155107 (2009) [60] Huang, B., Symonds, N.O., Lilienfeld, O.A.v.: In: Andreoni, W., Yip, S. (eds.) Quantum Machine Learning in Chemistry and Materials, pp. 1–27. Springer, Cham (2018) [61] Batzner, S., et al.: E(3)-equivariant graph neural networks for data- eﬃcient and accurate interatomic potentials. Nature Communications 13(1), 2453 (2022) [62] Tang, W., Sanville, E., Henkelman, G.: A grid-based bader analysis algo- rithm without lattice bias. J. Phys.: Condens. Matter 21(8), 084204 (2009) [63] Jung, R., Metzger, M., Maglia, F., Stinner, C., Gasteiger, H.A.: Oxy- gen release and its eﬀect on the cycling stability of linixmnycozo2 (nmc) cathode materials for li-ion batteries. J. Electrochem. Soc. 164(7), 1361 (2017) [64] Arnaldsson, A., et al.: Code: Bader Charge Analysis. http://theory.cm. utexas.edu/henkelman/code/bader/. version 1.04 [65] Lu, T., Chen, Q.: Interaction region indicator: A simple real space func- tion clearly revealing both chemical bonds and weak interactions**. Chemistry–Methods 1(5), 231–239 (2021) [66] de Silva, P., Corminboeuf, C.: Simultaneous visualization of covalent and", "page": 25, "position": 0}
