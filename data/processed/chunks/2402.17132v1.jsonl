{"chunk_id": "2402.17132v1_p0_c0", "doc_id": "2402.17132v1", "text": "A Bayesian Committee Machine Potential for Organic Nitrogen Compounds Hyun Gyu Park,1 Soohaeng Yoo Willow,1 D. ChangMo Yang,1, ∗and Chang Woo Myung1, † 1Department of Energy Science, Sungkyunkwan University, Seobu-ro 2066, Suwon, 16419, Korea (Dated: February 28, 2024) Large-scale computer simulations of chemical atoms are used in a wide range of applications, including batteries, drugs, and more. However, there is a problem with efficiency as it takes a long time due to the large amount of calculation. To solve these problems, machine learning interatomic potential (ML-IAP) technology is attracting attention as an alternative. ML-IAP not only has high accuracy by faithfully expressing the density functional theory (DFT), but also has the advantage of low computational cost. However, there is a problem that the potential energy changes significantly depending on the environment of each atom, and expansion to a wide range of compounds within a single model is still difficult to build in the case of a kernel-based model. To solve this problem, we would like to develop a universal ML-IAP using this active Bayesian Committee Machine (BCM) potential methodology for carbon-nitrogen-hydrogen (CNH) with various compositions. ML models are trained and generated through first-principles calculations and molecular dynamics simulations for molecules with only CNH. Using long amine structures to test an ML model trained only with short chains, the results show excellent consistency with DFT calculations. Consequently, machine learning-based models for organic molecules not only demonstrate the ability to accurately describe various physical properties but also hold promise for investigating a broad spectrum of diverse materials systems. INTRODUCTION Rapid advances in CPU and GPU parallelism as well as high-throughput cluster network architectures have enabled researchers and corporations worldwide to em- ploy numerous algorithmic innovations, resulting in a variety of applications in biology, physics, climate sci- ence, and chemistry at multiple scales [1–3]. Traditional methods such as Optimized Potentials for Liquid Simu- lations (OPLS) [4], Chemistry at Harvard Macromolecu- lar Mechanics (CHARMM) [5], Assisted Model Building with Energy Refinement (AMBER) [6], embedded-atom method (EAM) [7], reactive bond-order (ReaxFF) [8], charge-optimized many-body (COMB) [9], and other widely used classical force-fields or interatomic potentials", "page": 0, "position": 0}
{"chunk_id": "2402.17132v1_p0_c1", "doc_id": "2402.17132v1", "text": "numerous algorithmic innovations, resulting in a variety of applications in biology, physics, climate sci- ence, and chemistry at multiple scales [1–3]. Traditional methods such as Optimized Potentials for Liquid Simu- lations (OPLS) [4], Chemistry at Harvard Macromolecu- lar Mechanics (CHARMM) [5], Assisted Model Building with Energy Refinement (AMBER) [6], embedded-atom method (EAM) [7], reactive bond-order (ReaxFF) [8], charge-optimized many-body (COMB) [9], and other widely used classical force-fields or interatomic potentials enable large-scale simulations that are not feasible with ab initio calculations. They also have the advantage of reproducing some experimental data [10]. However, clas- sical force-fields (FF) frequently demonstrate limitations in representing helicity and noncovalent intermolecular interactions [10, 11]. Additionally, they often struggle to accurately describe correct structures and all associ- ated properties [12, 13]. Consequently, their accuracy is constrained by these shortcomings, particularly in de- picting various properties with correct structures simul- taneously [14]. Moreover, fitting reactive force-fields ne- cessitates prior knowledge of the reaction networks to be simulated, significantly constraining predictive capa- bility and potentially introducing human bias regard- ing the reactions that proceed [15]. An alternative ap- proach, utilizing ab initio data-driven ML-IAP methods, has been developed to address the issues and limitations of the FF method. This enables the accurate represen- tation of the potential energy surface (PES) [16, 17]. Over the last two decades, the application of ML-IAP in materials simulations has been proven highly success- ful, aiming to address the gap in speed, accuracy, and generality [15]. These include various machine learning techniques such as Neural Networks (NNs) [18], Artifi- cial Neural Network Methods (ANN) [19], Graph Neu- ral Network Potentials (GNN) [20], Gaussian Approx- imation Potentials (GAP) [21], Gradient Domain Ma- chine Learning (GDML) [22], and Sparse Gaussian Pro- cess Regression (SGPR) [23]. These methodologies have gained considerable traction not only in the field of phys- ical chemistry but also in various other applied disci- plines [23, 24]. While NNs are particularly effective at handling broad data sets, they have a huge number of optimizable parameters and require big data to avoid overfitting [25]. In contrast, Kernel-based methods show excellent performance in training", "page": 0, "position": 1}
{"chunk_id": "2402.17132v1_p0_c2", "doc_id": "2402.17132v1", "text": "(GDML) [22], and Sparse Gaussian Pro- cess Regression (SGPR) [23]. These methodologies have gained considerable traction not only in the field of phys- ical chemistry but also in various other applied disci- plines [23, 24]. While NNs are particularly effective at handling broad data sets, they have a huge number of optimizable parameters and require big data to avoid overfitting [25]. In contrast, Kernel-based methods show excellent performance in training and adapting on the fly using smaller datasets. However, the computational cost for training scales by O(n3) depending on the data size n [23, 25]. Consequently, to improve efficiency, the SGPR technique has been developed, utilizing subsets of data [23]. However, SGPR continues to face diffi- culties with a large number of inducing points m, ex- hibiting computational demands of O(nm2) [26, 27]. To overcome these challenges, researchers are exploring ag- gregation models such as Bayesian Committee Machines (BCMs) [26–29]. The BCM model combines predic- tions from submodels trained on different sections using SGPR with computational scaling of O(nm2). Further- more, it is anticipated that the scalability of ML-IAP will be enhanced through the extension of existing Gaus- sian Process (GP) learning methodologies, employing the Bayesian approach. In this study, we propose a highly reliable ML-IAP for organic compounds containing car- bon, nitrogen, and hydrogen, utilizing the SGPR algo- rithm and the BCM model. The training set consists of compounds containing only carbon, nitrogen, and hy- drogen, such as amines, alkaloids, azoles, cyanides, hy- arXiv:2402.17132v1 [cond-mat.mtrl-sci] 27 Feb 2024", "page": 0, "position": 2}
{"chunk_id": "2402.17132v1_p1_c0", "doc_id": "2402.17132v1", "text": "2 drazines, imidazoles, nitriles and pyridines. The model was tested using amine long chains (C21NH45), demon- strating high accuracy despite being trained on short- chain amines. The results of physical property calcula- tions using ML-IAP were similar to those obtained from DFT, confirming the excellence of the SGPR algorithm and BCM model. Our approach represents a significant advancement in the development of general-purpose ML- IAPs for organic compounds. THEORY Sparse Gaussian Process Regression potential ML-IAPs are commonly defined by incorporating all atoms within a defined cutoff radius. In a configuration x consisting of N atoms, a series of descriptors is gener- ated, represented as descriptor x = {ρi}N i=1, with each ρi dependent on the local chemical environment (LCE) of atom i within the cutoff radius. The ML-IAP energy be- comes additive across local chemical environment (LCE) ρi as E(x) = N X i=1 ϵ(ρi). (1) where ϵ represents a fictional energy arising from the in- teractions between atom i and its neighboring atoms. In kernel-based regression methods, ϵ is expressed as ϵ(ρ) = m X j=1 K(ρi, χj)ωj, (2) where the matrices are represented with bold fonts, z = {χj}m j=1 denotes the set of reference/inducing descrip- tors, w represents the vector of weights for the inducing descriptors, and K signifies a similarity/covariance ker- nel. The weights are determined to ensure the accurate reproduction of potential energy and forces for a given set of ab initio data X = {xk}n k=1, and their calculation is contingent upon the regression algorithm employed. In SGPR [23], w = (σ2kmm + kT nmknm)−1kT nmY (3) kmm and knm denote the interinducing and data- inducing covariance matrices, respectively. Y signifies the data potential energies (and forces), while σ rep- resents the noise hyperparameter. The noise scale σ, along with any hyperparameters within the kernel K, are fine-tuned to maximize the likelihood of the energy data. In defining the similarity kernel, we employ a mod- ified version of the smooth overlap of atomic positions (SOAP) [23, 30]. When utilizing existing data, the in- ducing descriptors are drawn from the dataset. However, in on-the-fly learning,", "page": 1, "position": 0}
{"chunk_id": "2402.17132v1_p1_c1", "doc_id": "2402.17132v1", "text": "energies (and forces), while σ rep- resents the noise hyperparameter. The noise scale σ, along with any hyperparameters within the kernel K, are fine-tuned to maximize the likelihood of the energy data. In defining the similarity kernel, we employ a mod- ified version of the smooth overlap of atomic positions (SOAP) [23, 30]. When utilizing existing data, the in- ducing descriptors are drawn from the dataset. However, in on-the-fly learning, both the data and inducing de- scriptors are sampled simultaneously during molecular dynamics (MD). Bayesian methods utilize the predictive variance as a threshold for sampling new data or induc- ing descriptors. Within SGPR, the predictive variance for ϵ(ρ) is determined as ν(ρ) = kρρ −kρmk−1 mmkT ρm + σ2kρm(σ2kmm + kT nm)−1kT ρm (4) To compute w at Eq. (3), we can transform this equa- tion into a linear system to bypass the matrix inversion (σ2kmm+kT nmknm)−1 [23]. However, in Eq. (4), this ma- trix inversion is explicitly required, a process that can be both numerically unstable and computationally expen- sive, particularly for on-the-fly learning [31]. Hence, in certain approximations, the third term in this equation is disregarded, potentially leading to slightly suboptimal sampling outcomes. Bayesian Committee Machine potential For the purpose of enhancing memory efficiency and improving the parallelization of the SGPR algorithm, we adopted the BCM [30] as an innovative method to amal- gamate SGPR-MLPs, each of which was trained inde- pendently on distinct subsets, as depicted in Fig. 1. By leveraging the BCM algorithm, we approximate the pre- dicted potential energy as follows: E ≈ˆS βα σ2α Eα (5) The calculation of the potential energy Eα involves the utilization of the α-th subset (with Rn∈α and zα = {χj}j∈α), along with the weight vector wα corresponding to the α-th sub SGPR model. This energy is described as: Eα(R) ≈ X i,(j∈mα) wαjK(ρi, χj). (6) We formulate the active BCM potential by selectively sampling the LCE, focusing on instances where the co- variance loss s(ρi) exceeds a predefined threshold. s(ρi) = K(ρi, ρi) −kρimk−1 mmkT ρim (7) The weighting of the α-th committee prediction relies on the utilization", "page": 1, "position": 1}
{"chunk_id": "2402.17132v1_p2_c0", "doc_id": "2402.17132v1", "text": "3 FIG. 1. The schematic represents the Bayesian committee machine potential. Following the division of the entire dataset into distinct subsets based on classification, training of SGPR-based MLPs occurs within each subset. Through this process, estimation of the universal MLP for the entire dataset is facilitated via the BCM. βα = −log(σ2 α). The final normalization factor ˆS is thus given: ˆS = \" p X α βα σ2α #−1 (9) The BCM combined with SGPR provides a significant advantage by allowing operation with a reduced (n/p)- dimensional training dataset and an (m/p)-dimensional inducing set, as opposed to the original n-dimensional training dataset and m-dimensional inducing set, where n = P α nα and m = P α mα. As a result, the computa- tional expense in BCM-based SGPR shifts from O(nm2) to pO( nm2 p3 ). Additionally, the BCM offers the advantage of giving more weight to a particular subset that closely resembles a test configuration x∗, thereby exerting a more substantial impact on energy, forces, and stress. Computational Details All ab initio calculations are conducted using the Vienna ab initio simulation package (VASP) ver- sion 6.0 [32], which employs the projector augmented- wave (PAW) [33] method within the framework of den- sity functional theory (DFT). The calculations utilize the Perdew-Burke-Ernzerhof (PBE) generalized gradient ap- proximation (GGA) functionals [34]. The kinetic-energy cutoff was set to 500 eV. The con- vergence criterion for the electronic energy difference was set to 105 eV. The Brillouin zone was sampled using the Γ-point. Using the atomic simulation environment pack- age (ASE) [35], NVT MD simulations were performed using Nos´e–Hoover thermostat and Parrinello-Rahman dynamics to explore different conformations of a given molecule. The MD simulations were conducted for 3-6 ps at 300 K, utilizing a time step of 0.5 fs. To assess the accuracy of ML-IAPs, we employ the coefficient of deter- mination, denoted by R2, which is defined as follows: R2 = 1 − P i(fi −˜fi)2 P i(f −¯fi)2 (10) The Python package AutoForce [36], integrated with the ASE, was employed for on-the-fly generation of SGPR models and learning of the PES.", "page": 2, "position": 0}
{"chunk_id": "2402.17132v1_p2_c1", "doc_id": "2402.17132v1", "text": "simulations were conducted for 3-6 ps at 300 K, utilizing a time step of 0.5 fs. To assess the accuracy of ML-IAPs, we employ the coefficient of deter- mination, denoted by R2, which is defined as follows: R2 = 1 − P i(fi −˜fi)2 P i(f −¯fi)2 (10) The Python package AutoForce [36], integrated with the ASE, was employed for on-the-fly generation of SGPR models and learning of the PES. RESULTS The ML-IAP model for each molecular category is gen- erated on the fly through a short MD simulation (Total step: 3 ps, step: 0.25 fs) of that molecule at 300 K, as shown in Fig. 2. Fig. 2 shows the training procedure for basic amine molecules. After each MD simulation, the model is updated and used as the basis for the next itera- tion. Among the many data sets in the amine group, ML calculations were performed starting with Butylamine. The ML potential energy is represented by the gray line, the red circle denotes the outcome of DFT calculation, and the blue circle correspondings to the DFT calcula- tion conducted every 10,000 steps to assess the variance with the SGPR ML calculation. In Butylamine in Fig. 2, many red circles were found in the early steps. This is because the uncertainty value (calculation setting value) was higher than that during ML training. This is the pro- cess of training the model as DFT calculations are run. In the middle step, fewer red circles were confirmed than in the initial step. This means that the model is trained in the initial step and the ML calculation is progress- ing below the calculation setting value. After calculating butylamine, propylamine was trained. Propylamine can be found in many red circles in the initial steps. This is because propylamine has a different structure from buty- lamine. However, it was confirmed that it gradually de- creased as the calculation progressed. Amylamine was calculated after calculating propylamine. Fewer red cir-", "page": 2, "position": 1}
{"chunk_id": "2402.17132v1_p3_c0", "doc_id": "2402.17132v1", "text": "4 FIG. 2. Initial steps for on-the-fly MLMD for elementary amine molecules. Red bullets indicate the ab initio potential energy of the configurations sampled by the SGPR model. Blue bullets are for single-point ab initio tests. cles were visible than the other two compounds because the ML model was trained on it. When calculating Octy- lamine, the red circle did not appear. This is because ML model training went smoothly. After ML training, 981, 17, 52, 39, 17, 34 and 34 ab initio samples have been ac- cumulated for amine, azole, alkaloid, cyanide, hydrazine, imidazole and nitrile, respectively. Table I shows the performance of the ML-IAP model on these train sets, showing metrics such as mean absolute error (MAE). Similar MD simulations are repeated without immedi- ate training to generate independent test sets. To assess the performance of the developed ML-IAP model, MD simulations (without real-time training) were carried out at 300 K using amine long chains (C21NH45). The amine group dataset depicted in Fig. S1(a) was employed to train the ML-IAP model, while Fig. S1(b) illustrates the amine long chains used for testing the ML-IAP model. The performance of the amine model is summarized in TABLE S1, which shows a high level of transferability. To ensure the thoroughness of the ML-IAP model train- ing, DFT calculations were conducted at regular intervals of 2.5 ps. The MAE was computed for both DFT and ML calculations at each 2.5 ps interval, revealing con- sistently low values. This observation strongly suggests that the model training process proceeded smoothly and effectively, validating the robustness and reliability of the ML-IAP model. Fig. 3 depicts the MD test outcomes of the ML-IAP model constructed using (C21NH45) data. The blue line represents the ML test results, while the red circles denote the DFT calculation results obtained at 2.5 ps intervals. Initially, disparities between the ML test and DFT calculations were observed. However, as the ML test progressed, the discrepancy in total potential energy gradually diminished. This observation confirms the outstanding performance of the ML-IAP model de- veloped using the amine group dataset. TABLE I. Testing the", "page": 3, "position": 0}
{"chunk_id": "2402.17132v1_p3_c1", "doc_id": "2402.17132v1", "text": "(C21NH45) data. The blue line represents the ML test results, while the red circles denote the DFT calculation results obtained at 2.5 ps intervals. Initially, disparities between the ML test and DFT calculations were observed. However, as the ML test progressed, the discrepancy in total potential energy gradually diminished. This observation confirms the outstanding performance of the ML-IAP model de- veloped using the amine group dataset. TABLE I. Testing the Expert Models Group Number of test samples Energy MAE (meV) Force MAE (eV/˚A) amine 981 0.184 0.099 azole 17 0.036 0.091 alkaloid 52 0.242 0.120 cyanide 39 0.052 0.099 hydrazine 17 0.104 0.114 imidazole 34 0.056 0.097 nitrile 99 0.129 0.099 DISCUSSION In this study, ML-IAPs acquired through the SGPR algorithm are utilized to predict the molecular behav- ior of CNH compounds. Expert models are developed for distinct CNH groups via on-the-fly ML, which are subsequently amalgamated to form universal ML-IAPs. These universal ML-IAPs, encompassing crucial poten- tial energy surfaces, demonstrate exceptional transfer- ability across diverse CNH systems. Collectively, these findings underscore the efficient generation of universal models from individually trained atomic potentials tar- geting various subgroup configurations. This approach", "page": 3, "position": 1}
{"chunk_id": "2402.17132v1_p4_c0", "doc_id": "2402.17132v1", "text": "5 FIG. 3. Initial steps for on-the-fly MLMD for elementary amine molecules. Red bullets indicate the ab initio potential energy of the configurations sampled by the SGPR model. Blue bullets are for single-point ab initio tests. presents a novel opportunity to create universal mod- els for multi-component systems, a capability previously unattainable with other ML methodologies. Moreover, the study emphasizes the effectiveness of the combined model (BCM) generated through kernel-based techniques like SGPR, offering heightened generality and transfer- ability. The utilization of BCM signifies a more robust and reliable framework for broader applications within the realm of ML-IAP. ACKNOWLEDGEMENTS SYW and CWM acknowledge the support from the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. RS2023- 00222245). CWM acknowledges the support from the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. NRF- 2022R1C1C1010605). The authors are grateful for: the computational support from the Korea Institute of Science and Technology Information (KISTI) for the Nurion cluster (KSC-2023-CRE-0059,KSC-2023-CRE- 0355,KSC-2023-CRE-0454). ∗dcyang@skku.edu † cwmyung@skku.edu [1] B. J. Alder and T. E. Wainwright, J. Chem. Phys. 31, 459 (1959). [2] J. A. McCammon, B. R. Gelin, and M. Karplus, Nature 267, 585 (1977). [3] I. N. M. Le, O. Kiss, J. Schuhmacher, I. Tavernelli, and F. Tacchino, arXiv preprint arXiv:2311.11362 (2023). [4] W. L. Jorgensen, J. D. Madura, and C. J. Swenson, J. Am. Chem. Soc. 106, 6638 (1984). [5] B. R. Brooks, C. L. Brooks III, A. D. Mackerell Jr, L. Nilsson, R. J. Petrella, B. Roux, Y. Won, G. Archon- tis, C. Bartels, S. Boresch, et al., J. Comput. Chem. 30, 1545 (2009). [6] W. D. Cornell, P. Cieplak, C. I. Bayly, I. R. Gould, K. M. Merz, D. M. Ferguson, D. C. Spellmeyer, T. Fox, J. W. Caldwell, and P. A. Kollman, J. Am. Chem. Soc. 117, 5179 (1995). [7] M. S. Daw and M. I. Baskes, Phys. Rev. Lett. 50, 1285 (1983). [8] A. C. Van Duin, S. Dasgupta, F. Lorant, and W. A. Goddard, J. Phys. Chem. A 105, 9396 (2001). [9] T. Liang, B. Devine, S. R.", "page": 4, "position": 0}
{"chunk_id": "2402.17132v1_p4_c1", "doc_id": "2402.17132v1", "text": "Bayly, I. R. Gould, K. M. Merz, D. M. Ferguson, D. C. Spellmeyer, T. Fox, J. W. Caldwell, and P. A. Kollman, J. Am. Chem. Soc. 117, 5179 (1995). [7] M. S. Daw and M. I. Baskes, Phys. Rev. Lett. 50, 1285 (1983). [8] A. C. Van Duin, S. Dasgupta, F. Lorant, and W. A. Goddard, J. Phys. Chem. A 105, 9396 (2001). [9] T. Liang, B. Devine, S. R. Phillpot, and S. B. Sinnott, J. Phys. Chem. A 116, 7976 (2012). [10] A. Hajibabaei, M. Ha, S. Pourasad, J. Kim, and K. S. Kim, J. Phys. Chem. A 125, 9414 (2021). [11] A. T. Tzanov, M. A. Cuendet, and M. E. Tuckerman, J. Phys. Chem. B 118, 6539 (2014). [12] N. D. Kondratyuk, G. E. Norman, and V. V. Stegailov, J. Chem. Phys. 145 (2016). [13] N. Kondratyuk, J. Phys.: Conf. Ser. 1385, 012048 (2019). [14] M. Ha, A. Hajibabaei, S. Pourasad, and K. S. Kim, ACS Phys. Chem. Au 2, 260 (2022). [15] S. Zhang, M. Mako´s, R. Jadrich, E. Kraka, K. Bar- ros, B. Nebgen, S. Tretiak, O. Isayev, N. Lubbers, R. Messerly, and J. Smith, “Exploring the frontiers of chemistry with a general reactive machine learning po- tential,” (2023). [16] O. T. Unke, S. Chmiela, H. E. Sauceda, M. Gastegger, I. Poltavsky, K. T. Schu¨utt, A. Tkatchenko, and K.-R. M¨uller, Chem. Rev. 121, 10142 (2021). [17] V. L. Deringer, A. P. Bart´ok, N. Bernstein, D. M. Wilkins, M. Ceriotti, and G. Cs´anyi, Chem. Rev. 121, 10073 (2021). [18] J. Behler and M. Parrinello, Phys. Rev. Lett. 98, 146401 (2007).", "page": 4, "position": 1}
{"chunk_id": "2402.17132v1_p5_c0", "doc_id": "2402.17132v1", "text": "6 [19] G. P. Pun, R. Batra, R. Ramprasad, and Y. Mishin, Nat. Commun. 10, 2339 (2019). [20] S. Batzner, A. Musaelian, L. Sun, M. Geiger, J. P. Mailoa, M. Kornbluth, N. Molinari, T. E. Smidt, and B. Kozinsky, Nat. Commun. 13, 2453 (2022). [21] A. P. Bart´ok, M. C. Payne, R. Kondor, and G. Cs´anyi, Phys. Rev. Lett. 104, 136403 (2010). [22] S. Chmiela, A. Tkatchenko, H. E. Sauceda, I. Poltavsky, K. T. Sch¨utt, and K.-R. M¨uller, Sci. Adv. 3, e1603015 (2017). [23] A. Hajibabaei, C. W. Myung, and K. S. Kim, Phys. Rev. B 103, 214102 (2021). [24] S. J. Hong, H. Chun, J. Lee, B.-H. Kim, M. H. Seo, J. Kang, and B. Han, J. Phys. Chem. Lett. 12, 6000 (2021). [25] A. Hajibabaei and K. S. Kim, J. Phys. Chem. Lett. 12, 8115 (2021). [26] S. Y. Willow and C. W. Myung, arXiv preprint arXiv:2402.06256 (2024). [27] S. Y. Willow, G. S. Kim, M. Ha, A. Hajibabaei, and C. W. Myung, “A sparse bayesian committee machine potential for hydrocarbons,” (2024), arXiv:2402.14497 [cond-mat.mtrl-sci]. [28] G. E. Hinton, Neural computation 14, 1771 (2002). [29] V. Tresp, Neural computation 12, 2719 (2000). [30] A. P. Bart´ok, R. Kondor, and G. Cs´anyi, Phys. Rev. B 87, 184115 (2013). [31] C. Williams, C. Rasmussen, A. Scwaighofer, and V. Tresp, Observations on the Nystr¨om Method for Gaus- sian Process Prediction, Technical Report of the Univer- sity of Edinburgh (University of Edinburgh, Edinburgh, UK, 2002). [32] G. Kresse and J. Furthm¨uller, Phys. Rev. B 54, 11169 (1996). [33] P. E. Bl¨ochl, Phys. Rev. B 50, 17953 (1994). [34] J. P. Perdew, K. Burke, and M. Ernzerhof, Phys. Rev. Lett. 77, 3865 (1996). [35] A. H. Larsen, J. J. Mortensen, J. Blomqvist, I. E. Castelli, R. Christensen, M. Du lak, J. Friis, M. N. Groves, B. Hammer, C. Hargus, et al., J. Phys.: Con- dens. Matter. 29, 273002 (2017). [36] AutoForce, “A Python Package for Sparse Gaussian Process Regression of the Ab-Initio Potential Energy Sur- face,” https://github.com/amirhajibabaei/AutoForce (2023).", "page": 5, "position": 0}
{"chunk_id": "2402.17132v1_p7_c0", "doc_id": "2402.17132v1", "text": "2 The following CNH groups were considered for the gas phase: • amine : methylamine, ethylamine, propylamine, butylamine, aniline, phenylamine, isopropy- lamine, pentanamine, amylamine, hexamine, methenamine, hexanamine, hexylamine, hep- tanamine, heptylamine octanamine, octylamine, decanamine, decylamine, cyclopropylamine, cyclopentylamine, cyclohexylamine, dimethylamine, ethylmethylamine, diethylamine, naph- thylamine, diphenylamine, toluenediamine, trimethylamine, triethylamine, triphenylamine, ethylenediamine, imipramine, desipramine, diethylenetriamine, dimethyltryptamine, tripe- lennamine, tetraethylenepentamine, 2-aminopyridine, 6-benzylaminopurine, trimipramine, m-phenylenediamine, pheniramine, tryptamine, 3-methyladenine, gramine, 1-methylhistamine, bis(3-aminopropyl)amine, n-methyltryptamine • alkaloid : nicotine, piperidine, sparteine, putrescine, spermidine, spermine, ormosanine, pip- tanthine, nornicotine, anabasine, anatabine, actinidine, histamine • azole : imidazole, dihydroimidazole, pyrrole, pyrazole, triazole, tetrazole, pentazole, benz- imidazole, bifonazole • cyanide : hydrogen-cyanide, n-phenylbenzimidoyl-cyanide, vinylidene-cyanide, cyano-cyanopyridine- cyanide, 2-anilino-1-methylethyl-cyanide, n-ethylpropanimidoyl-cyanide, 2-phenylethanimidoyl- cyanide, hexahydrocarbazole-cyanide • hydrzine : methylhydrazine, mebanazine, ethylhydrazine, dihydralazine, phenylhydrazine, pheniprazine, phenelzine, 1,1-dimethylhydrazine, 1,2-diphenylhydrazine, 1,2-diethylhydrazine, benzylhydrazine, benzophenone-hydrazone, 1-Methyl-1-phenylhydrazine • imidazole: imidazole, benzimidazole, bifonazole, 1-methylimidazole, 1-benzylimidazole, 1- benzyl-2-methylimidazole, 1,2-dimethylimidazole, 2-ethylimidazole, 2-phenylbenzimidazole, 2-ethylbenzimidazole, 2-ethyl-4-methylimidazole, 2-aminobenzimidazole, 2-methylimidazole, 2-methylbenzimidazole, 4-methylimidazole, 5-methylbenzimidazole, 5,6-dimethylbenzimidazole • nitile : adiponitrile, , acrylonitrile, acetonitrile, benzonitrile, butyronitrile, propionitrile, phenylacetonitrile, valeronitrile, heptanenitrile, 3-aminopropionitrile, 3-buteneitrile, 3-phenylpropionitrile, 3-pentenenitrile, 4-pentenenitrile, malononitrile, methacrylonitrile, isobutyronitrile, iso- valeronitrile, dodecanenitrile, o-tolunitrile, m-tolunitrile, stearonitrile, tridecanenitrile, trimethy- lacetonitrile", "page": 7, "position": 0}
