{"chunk_id": "2304.06043v1_p0_c0", "doc_id": "2304.06043v1", "text": "1 A Deep Learning Approach Towards Generating High-ﬁdelity Diverse Synthetic Battery Datasets Janamejaya Channegowda, Member, IEEE, Vageesh Maiya, Student Member, IEEE, and Chaitanya Lingaraj Member, IEEE, Abstract—Recent surge in the number of Electric Vehicles have created a need to develop inexpensive energy-dense Battery Stor- age Systems. Many countries across the planet have put in place concrete measures to reduce and subsequently limit the number of vehicles powered by fossil fuels. Lithium-ion based batteries are presently dominating the electric automotive sector. Energy research efforts are also focussed on accurate computation of State-of-Charge of such batteries to provide reliable vehicle range estimates. Although such estimation algorithms provide precise estimates, all such techniques available in literature presume availability of superior quality battery datasets. In reality, gaining access to proprietary battery usage datasets is very tough for battery scientists. Moreover, open access datasets lack the diverse battery charge/discharge patterns needed to build generalized models. Curating battery measurement data is time consuming and needs expensive equipment. To surmount such limited data scenarios, we introduce few Deep Learning-based methods to syn- thesize high-ﬁdelity battery datasets, these augmented synthetic datasets will help battery researchers build better estimation models in the presence of limited data. We have released the code and dataset used in the present approach to generate synthetic data. The battery data augmentation techniques introduced here will alleviate limited battery dataset challenges. Index Terms—Lithium Ion batteries, Energy Storage, Synthetic Data, Deep Learning I. INTRODUCTION Driven by the pressing need to curb life threatening pol- lutants emitted by the transportation domain, there has been a massive push by industries and state heads to electrify various transportation modes. Electric Vehicles (EVs) have garnered signiﬁcant interest as a reliable replacement of fossil fuel vehicles. Li-ion batteries (LiBs) have a proven property of being energy dense with minimal self discharge [1], [2]. The Battery Management System (BMS), included in the battery pack, is provided to protect the batteries [3]. State- of-Charge (SOC) of batteries is a crucial metric to gauge the remaining charge present in the battery pack. SOC is expressed as the percentage of charge remaining in the battery", "page": 0, "position": 0}
{"chunk_id": "2304.06043v1_p0_c1", "doc_id": "2304.06043v1", "text": "replacement of fossil fuel vehicles. Li-ion batteries (LiBs) have a proven property of being energy dense with minimal self discharge [1], [2]. The Battery Management System (BMS), included in the battery pack, is provided to protect the batteries [3]. State- of-Charge (SOC) of batteries is a crucial metric to gauge the remaining charge present in the battery pack. SOC is expressed as the percentage of charge remaining in the battery to the maximum available battery capacity [4]. To provide dependable mileage of the EV, computing SOC is crucial. A more straightforward measurement of SOC is difﬁcult and in most cases SOC is determined from other battery parameters (Voltage, Temperature and Current) [5]. Precise SOC estimates rely on curated battery dataset. A. Literature Review The SOC determination techniques available in literature can be classiﬁed into the following categories [6], [7], [8], [9]: • Coulomb Counting Approach • Open Circuit Voltage (OCV) method • Physics model-based computation • Data-driven models This paper is focused on the data-driven model approach, a new paradigm, which works with massive amounts of battery dataset [9]. The key difference between data-driven and other battery models is the reliance on high-quality curated dataset. The models rely on the inference provided by machine learn- ing or deep learning techniques. The recent growth of such learning techniques has enabled battery researchers to learn and highlight the interdependencies between variables of inter- est. There have been few State-of-Charge estimators discussed in literature [10], [11]. Data-driven techniques greatly differ from conventional SOC estimators with the computation being performed with a uniform set of neural network parameters. Although such data-driven techniques are known to perform exceptionally well as SOC estimators, they are not devoid of implementation complexity. Some of the issues include, scalability problems and the assumption that all train, test and validation dataset originate from identical distributions. This limits the generalizability of these algorithms. A Deep Neural Network (DNN) is an augmented variant of the Artiﬁcial Neural Network (ANN), which means a DNN has much more layers compared to ANN. DNN has performed exceptionally well in the area of Computer Vision [12]", "page": 0, "position": 1}
{"chunk_id": "2304.06043v1_p0_c2", "doc_id": "2304.06043v1", "text": "devoid of implementation complexity. Some of the issues include, scalability problems and the assumption that all train, test and validation dataset originate from identical distributions. This limits the generalizability of these algorithms. A Deep Neural Network (DNN) is an augmented variant of the Artiﬁcial Neural Network (ANN), which means a DNN has much more layers compared to ANN. DNN has performed exceptionally well in the area of Computer Vision [12] and language translation [13]. However, deep neural networks have not been extensively explored for SOC estimation. Use of Long Short- Term Memory (LSTM) network have been trained and perfor- mance has been evaluated on various vehicle drive cycle data [14]. A Multilayer DNN has also been used for SOC prediction [15]. DNN has been a promising methodology which has been employed towards SOC estimation due to its ability to work with non-linear input data [16], [17]. All data-driven models assume access to completely labelled datasets but, privacy concerns limit the access of proprietary datasets to battery researchers. Limited dataset greatly restrict the advancement of precise battery degradation and SOC estimation algorithms. B. Motivation Behind Current Research Trajectory Most deep learning techniques heavily depend on supe- rior quality labelled datasets to provide useful estimates. In most cases such dataset is unavailable due to the expense and time involved in gathering such dataset. In literature, most research efforts have focussed on deriving insights from existing rudimentary open access datasets, in reality, man- ufacturers rarely share proprietary datasets with researchers. arXiv:2304.06043v1 [cs.LG] 9 Apr 2023", "page": 0, "position": 2}
{"chunk_id": "2304.06043v1_p1_c0", "doc_id": "2304.06043v1", "text": "2 0 1000 2000 3000 4000 5000 Time (s) 3 3.5 4 Voltage (V) Voltage 0 1000 2000 3000 4000 5000 Time (s) -20 -10 0 Amperes (A) Current 0 1000 2000 3000 4000 5000 Time (s) 26 28 30 32 Temperature (deg C) Temperature 0 1000 2000 3000 4000 5000 Time (s) 0 50 100 SOC (%) State of Charge Fig. 1: Typical battery discharge characteristics of a Panasonic 18650 cell [18] Battery Dataset II Limited Battery Dataset Battery Dataset I Battery Dataset III Extract Features Embedding Space Learned Structure + Data Augmentation Synthetic Data Generation Fig. 2: An illustration of basic synthetic data generation methodology followed in any data-driven technique The deep learning methods mentioned here help to generate reliable synthetic battery parameter datasets. The produced data possess a high degree of similarity with the generated data. This paper addresses the key challenge of sparse datasets. The synthetic data greatly simpliﬁes computational effort and saves experimentation expenses. C. Contributions Some of the key contributions in tackling the issue of sparse datasets has been listed here: 1) A deep learning based synthetic data generation technique is introduced to surmount sparse data challenges 2) This work provides comprehensive comparison of various state-of-the-art deep learning techniques, relevant to time series data, to produce high-ﬁdelity heterogeneous battery datasets 3) The goal is to release code and dataset used in this present approach to enable researchers reproduce and build upon our results The paper is organised as follows, fundamentals of deep learning architecture employed to produce synthetic data is described in Section II, details of the experiments performed to evaluate the synthetic data are provided in Section III. Results are discussed and inferences are elucidated in Section III followed by conclusion in Section IV. II. DEEP LEARNING-BASED SYNTHETIC DATA GENERATORS Battery measurement datasets typically contain variations of key battery parameters such as Voltage, Current, Temperature and SOC captured over multiple charge and discharge cycles. One such discharge proﬁle for an 18650 battery is shown in Fig. 1. Battery measurement data collected over several cycles will enable researchers to devise accurate methodologies to estimate", "page": 1, "position": 0}
{"chunk_id": "2304.06043v1_p1_c1", "doc_id": "2304.06043v1", "text": "are elucidated in Section III followed by conclusion in Section IV. II. DEEP LEARNING-BASED SYNTHETIC DATA GENERATORS Battery measurement datasets typically contain variations of key battery parameters such as Voltage, Current, Temperature and SOC captured over multiple charge and discharge cycles. One such discharge proﬁle for an 18650 battery is shown in Fig. 1. Battery measurement data collected over several cycles will enable researchers to devise accurate methodologies to estimate battery capacity fade over the life cycle of the cell. Lack of such curated dataset warrants the use of synthetic data generators. Any synthetic data generation technique fol- lows an encoder-decoder architecture style as illustrated in Fig. 2. The end goal in all such methods is to perform data augmentation, which helps to increase model training data by producing similar copies of groundtruth values. There has been signiﬁcant progress in developing accurate deep learning time series based estimation approaches over the past few years, we evaluate the generation of synthetic data on primarily three state-of-the-art methods as listed in this section. A. Autoregressive Recurrent Networks Estimation (DeepAR) Multiple research attempts have been made in the past to employ fundamental neural network based architectures for forecasting future datapoints [21],[22]. Recently Nikolaos et al [23] used focussed forecasting by using intermittent datapoints but with unsatisfactory results. Most of the initial work involved univariate data and model ﬁt was evaluated on such time series data separately [24], [25]. These models till date have not been employed for data augmentation purposes. Among such models Recurrent Neural Networks have been found to be very useful, especially in signal processing and natural language processing applications [26], [27]. The most useful features of long-sequence forecasting [20] that are relevant to synthetic time-series data are as follows: • Overall approximate prediction distribution is considered to obtain accurate variable estimates • This approach uses a negative binomial likelihood to provide better estimates of variables of interest The entire procedure is shown in Fig. 3 which shows the training & estimation methods. The ﬁnal goal is to derive the conditional zi,t (i is the sequence under consideration at instant t)", "page": 1, "position": 1}
{"chunk_id": "2304.06043v1_p1_c2", "doc_id": "2304.06043v1", "text": "are relevant to synthetic time-series data are as follows: • Overall approximate prediction distribution is considered to obtain accurate variable estimates • This approach uses a negative binomial likelihood to provide better estimates of variables of interest The entire procedure is shown in Fig. 3 which shows the training & estimation methods. The ﬁnal goal is to derive the conditional zi,t (i is the sequence under consideration at instant t) as shown in Equation 1. P(zi,t0:T |zi,1:t0−1, xi,1:T ) (1) The distribution shown in Equation 1 is tied with the series [zi,t0, zi,t0+1, . . . , zi,T ] := zi,t0:T , in this case, it is assumed that the earlier values are known ([zi,1, . . . , zi,t0−2, zi,t0−1] := zi,1:t0−1), we also consider the covariates (xi,1:T ) to be known for every time step. In this approach [t0, T] is the prediction window and [1, t0 −1] is the conditioning window. The training period guarantees that both window ranges are in the past and zi,t is perceived. The predicted sequence, zi,t, exists only in the conditioning interval. The entire model is based on the recurrent neu- ral network architecture [26], [27]. The foundation of the network means that the value at the very last step zi,t−1", "page": 1, "position": 2}
{"chunk_id": "2304.06043v1_p2_c0", "doc_id": "2304.06043v1", "text": "3 Fig. 3: Training procedure is depicted in (a), the inputs to the model are the temporal info t, data covariates xi,t, historical targets zi,t−1 and past outputs hi,t−1. Subsequently, the network result hi,t = h(hi,t−1, zi,t−1, xi,t, Θ) is employed to compute θi,t = θ(hi,t, Θ) of ℓ(z|θ). The estimation methodology is shown in (b), the input in this case is past data zi,t in the duration t < t0. For t ≥t0 a trial ˆzi,t ∼ℓ(·|θi,t) which is provided to the next iteration. This process is continued until the end of the estimated time series t = t0 + T, thus generating a synthetic sequence into the future [20] is provided as input and also as the recurrent value. The autoregressive portion implies that the previous neural network output (hi,t−1) is provided as input towards the next time step. In the experiments conducted on the battery datasets in this paper, the autoregressive recurrent network was employed for conditioning and prediction interval. B. Neural Basis Expansion Analysis (N-BEATS) Another recent deep neural architecture is the Neural basis expansion analysis approach for long-sequence time-series forecasting [28]. The entire architecture is shown in Fig. 4. The model takes as input ⃗x and the output being b⃗x & b⃗y. The input, ⃗x, consists of a historical window, the total sequence length of this window can contain many forecast horizons H. The estimated values, b⃗y, contains the estimated length H length & b⃗x. The primary blocks of this architecture are: • A fully connected network generating the forward pre- dictor (θf) & backward predictor (θb) • The Forward layer (gf) & the backward layer (gb) Ground Truth Predictions Fig. 4: Basic blocks of the network used in Neural basis expansion analysis. The calculation of forward (θf) & backward (θb) coef- ﬁcients is performed. A single stack has many layers which have similar gb & gf between them. The Fully Connected (FC) layers are described by the fol- lowing equations: ⃗h1 = FC1(⃗x) (2) ⃗h2 = FC2(⃗h1) (3) ⃗h3 = FC3(⃗h2) (4) ⃗h4 = FC4(⃗h3) (5) θb = Linearb(⃗h4) (6) θf = Linearf(⃗h4) (7)", "page": 2, "position": 0}
{"chunk_id": "2304.06043v1_p2_c1", "doc_id": "2304.06043v1", "text": "the network used in Neural basis expansion analysis. The calculation of forward (θf) & backward (θb) coef- ﬁcients is performed. A single stack has many layers which have similar gb & gf between them. The Fully Connected (FC) layers are described by the fol- lowing equations: ⃗h1 = FC1(⃗x) (2) ⃗h2 = FC2(⃗h1) (3) ⃗h3 = FC3(⃗h2) (4) ⃗h4 = FC4(⃗h3) (5) θb = Linearb(⃗h4) (6) θf = Linearf(⃗h4) (7) Here, Linear depicts the projection layer, this signiﬁes that θf = ⃗W f⃗h4, the entire purpose of this module is to calculate coefﬁcients θf, this coefﬁcient enables optimization of the estimates b⃗y. Ultimately, the coefﬁcients θb are provided as inputs to the backward basis layer (gb) to produce the estimate of ⃗x. The next block of the architecture links θf & θb to the estimates throughout the layers b⃗y = gf(θf) & b⃗x = gb(θb ℓ), the equation describing these operations are listed in Equation 8. b⃗yℓ= dim(θf ℓ) X i=1 θf ℓ,i⃗vf i , b⃗xℓ= dim(θb ℓ) X i=1 θb ℓ,i⃗vb i . (8) C. Deep Temporal Convolutional Network (DeepTCN) The Deep Temporal Convolutional Network (DeepTCN), is a non-autoregressive estimation technique, this approach has proved to be particularly useful due to the following advantages [29]: 1) Furnishes parametric & non-parametric means to estimate probability density 2) Ability to learn dormant correlation between multivari- ate series which equips it to manage advanced battery datasets", "page": 2, "position": 1}
{"chunk_id": "2304.06043v1_p3_c0", "doc_id": "2304.06043v1", "text": "4 {X, y}t−k... ... ... ... ... ... ... ...{X, y}t−6...{X, y}t−4...{X, y}t−2... {X, y}t Xt+1 ... Xt+Ω ... + + Input Hidden Dilation=1 Hidden Dilation=2 Hidden Dilation=4 Output Dilation=8 ˆyt+1 ˆyt+Ω ht Encoder Decoder resnet-v dense Fig. 5: DeepTCN algorithm, the encoder consists of stacked dilated nets needed to embed all the long sequence temporal information. The decoder has a variant of residual block & an output layer which is dense. The dense layer links the output of the residual block to the ﬁnal battery parameter estimates. 3) Provides ﬂexible means to include additional variables during estimation phase The complete architecture of DeepTCN is shown in Fig. 5, the encoder and decoder portions are illustrated in Fig. 6 and Fig. 7 respectively. In any time series estimation procedure the series can be described as y1:t = {y(i) 1:t}N i=1, the estimated series is denoted by y(t+1):(t+Ω) = {y(i) (t+1):(t+Ω)}N i=1, here N denotes the total number of time series, t denotes past datapoints & Ωis the estimation sequence. The chief aim is to estimate the entire distribution of future datapoints P \u0000y(t+1):(t+Ω)|y1:t \u0001 . The conventional generative architectures employed in time series analysis consider the joint probability of subsequent datapoints provided that we have access to historical data as depicted in the Equation. 9. P \u0000y(t+1):(t+Ω)|y1:t \u0001 = Ω Y ω=1 p(yt+ω|y1:t+ω−1), (9) Typically, generative models do not generalize well in all real- world applications, there is also accumulation of error for each prediction sequence, this is due to the fact that every estimated value is fed back as raw data to provide longer horizon estimates. In DeepTCN, the joint distribution of all estimates are observed right away. P \u0000y(t+1):(t+Ω)|y1:t \u0001 = Ω Y ω=1 p(yt+ω|y1:t). (10) It should also be noted that every time series sequence has components such as seasonality & trend , these characteristics are also important when predicting future estimates, in this case the covariates X(i) t+ω (here ω = 1, ..., Ω& i = 1, ..., N) which provides additional info for the estimation framework in Equation 10. Finally the entire joint distribution of", "page": 3, "position": 0}
{"chunk_id": "2304.06043v1_p3_c1", "doc_id": "2304.06043v1", "text": "\u0000y(t+1):(t+Ω)|y1:t \u0001 = Ω Y ω=1 p(yt+ω|y1:t). (10) It should also be noted that every time series sequence has components such as seasonality & trend , these characteristics are also important when predicting future estimates, in this case the covariates X(i) t+ω (here ω = 1, ..., Ω& i = 1, ..., N) which provides additional info for the estimation framework in Equation 10. Finally the entire joint distribution of predicted estimates are provided in Equation. 11. P \u0000y(t+1):(t+Ω)|y1:t \u0001 = Ω Y ω=1 p(yt+ω|y1:t, X(i) t+ω, i = 1, ..., N). (11) Dilated Conv Batch Norm ReLU Dilated Conv Batch Norm ReLU + Residual Block: (K,d) Fig. 6: Encoder of DeepTCN is shown here, every residual module has many layers of convolutions, the very ﬁrst layer is led by batch normalization & ReLU. In the encoder the output is fed as the input to the residual block and then by a ReLU activation function. Therefore the key challenge DeepTCN is able to answer is its ability to provide a suitable framework which includes past datapoints y1:t and covariates X(i) t+ω to provide better estimates. III. RESULTS AND DISCUSSION All the deep learning models described in the pa- per were used to produce synthetic data, we employed two publicly available datasets to evaluate all the esti- mation techniques described here. The implementation de- tails can be found here: https://github.com/vageeshmaiya/ Deep-Learning-based-Battery-Synthetic-Data. After the pre- processing step of data normalization, we focussed on battery voltage and battery capacity from a LiCoO2 battery [19] and a 18650 Panasonic battery [18]. The initial estimates of both voltage and capacity for DeeAR model can be found in Fig. 8 and Fig. 9. Subsequently we employed similar dataset to evaluate the N-BEATS and DeepTCN models as described in Fig. 10, Fig. 11, Fig. 12 and Fig. 13 respectively. We have used", "page": 3, "position": 1}
{"chunk_id": "2304.06043v1_p5_c0", "doc_id": "2304.06043v1", "text": "6 GroundTruth Prediction Fig. 12: DeepTCN Voltage estimates with horizon 30 GroundTruth Prediction Fig. 13: DeepTCN battery Capacity estimates with horizon 30 Mean Absolute Error (MAE) metrics described in Equation. 12 to observe the behaviour of these deep learning models over a range of horizon values as shown in Fig. 14 and Fig. 15. MAE = 1 n n X i=1 |y −ˆy| (12) In Equation. 12, y represents ground truth values & the esti- mated values are shown by ˆy. We can observe that DeepTCN provides the best performance across both datasets and for both battery voltage and battery capacity estimates for a varied horizon length. In general, battery voltage estimate error metrics increase overtime due to a lower voltage resolution of 0.1 V across different battery datasets. From this exhaustive analysis, we can consider deploying DeepTCN to provide high-ﬁdelity synthetic battery datasets during sparse dataset scenarios. IV. CONCLUSION Lack of abundant labelled battery dataset is a cause of con- cern when researchers are developing novel battery capacity and health estimation algorithms backed by deep learning. Present open access datasets do not contain the necessary diversity (various charge/discharge proﬁles) to develop gen- eralized estimation algorithms. The duration and the hardware cost involved in collecting battery measurements are also some of the major reasons for data scarcity. This paper compares and evaluates three state-of-the-art deep learning approaches to generate high quality battery synthetic datasets. All approaches were evaluated on two publicly available datasets and it was found that using Temporal Convolutional Neural Network provided the best performance for such data augmentation purposes. REFERENCES [1] R. Xiong, L. Li, and J. Tian, “Towards a smarter battery management system: A critical review on battery state of health monitoring methods,” J. Power Sources, vol. 405, pp. 18-29, Nov. 2018. [2] H. Zhang, Z. Mo, J. Wang, and Q. Miao, “Nonlinear-drifted fractional brownian motion with multiple hidden state variables for remaining useful life prediction of lithium-ion batteries,” IEEE Trans. Rel., vol. 69, no. 2, pp. 768-780, Jun. 2020. [3] H. Rahimi-Eichi, U. Ojha, F. Baronti, and M.-Y. Chow, “Battery management system: An overview of", "page": 5, "position": 0}
{"chunk_id": "2304.06043v1_p5_c1", "doc_id": "2304.06043v1", "text": "on battery state of health monitoring methods,” J. Power Sources, vol. 405, pp. 18-29, Nov. 2018. [2] H. Zhang, Z. Mo, J. Wang, and Q. Miao, “Nonlinear-drifted fractional brownian motion with multiple hidden state variables for remaining useful life prediction of lithium-ion batteries,” IEEE Trans. Rel., vol. 69, no. 2, pp. 768-780, Jun. 2020. [3] H. Rahimi-Eichi, U. Ojha, F. Baronti, and M.-Y. Chow, “Battery management system: An overview of its application in the smart grid and electric vehicles,” IEEE Ind. Electron. Mag., vol. 7, no. 2, pp. 4-16, Jun. 2013. [4] M. Berecibar, I. Gandiaga, I. Villarreal, N. Omar, J. Van Mierlo, and P. Van den Bossche, “Critical review of state of health estimation methods of Li-ion batteries for real applications,” Renew. Sustain. Energy Rev., vol. 56, pp. 572-587, Apr. 2016. [5] F. Yang, W. Li, C. Li, and Q. Miao, “State-of-charge estimation of lithium-ion batteries based on gated recurrent neural network,” Energy, vol. 175, pp. 66-75, May 2019. [6] M. A. Hannan, M. H. Lipu, A. Hussain, and A. Mohamed, “A review of lithium-ion battery state of charge estimation and management system in electric vehicle applications: Challenges and recommendations,” Renew. Sustain. Energy Rev., vol. 78, pp. 834-854, Oct. 2017. [7] R. Xiong, J. Cao, Q. Yu, H. He, and F. Sun, “Critical review on the battery state of charge estimation methods for electric vehicles,” IEEE Access, vol. 6, pp. 1832-1843, Dec. 2018. [8] X. Hu, S. E. Li, and Y. Yang, “Advanced machine learning approach for lithium-ion battery state estimation in electric vehicles,” IEEE Trans. Transport. Electriﬁc., vol. 2, no. 2, pp. 140-149, Jun. 2016. [9] D. N. How, M. A. Hannan, M. H. Lipu, and P. J. Ker, “State of charge estimation for lithium-ion batteries using model-based and data- driven methods: A review,” IEEE Access, vol. 7, pp. 136116-136136, Sep. 2019. [10] E. Chemali, P. J. Kollmeyer, M. Preindl, R. Ahmed, and A. Emadi, “Long short-term memory networks for accurate State-of-Charge esti- mation of Li-ion batteries,” IEEE Trans. Ind. Electron., vol. 65, no. 8, pp. 6730-6739, Aug. 2018. [11] G. Abbas, M. Nawaz, and F. Kamran, “Performance", "page": 5, "position": 1}
{"chunk_id": "2304.06043v1_p5_c2", "doc_id": "2304.06043v1", "text": "Ker, “State of charge estimation for lithium-ion batteries using model-based and data- driven methods: A review,” IEEE Access, vol. 7, pp. 136116-136136, Sep. 2019. [10] E. Chemali, P. J. Kollmeyer, M. Preindl, R. Ahmed, and A. Emadi, “Long short-term memory networks for accurate State-of-Charge esti- mation of Li-ion batteries,” IEEE Trans. Ind. Electron., vol. 65, no. 8, pp. 6730-6739, Aug. 2018. [11] G. Abbas, M. Nawaz, and F. Kamran, “Performance comparison of NARX & RNN-LSTM neural networks for LiFePO4 battery state of charge estimation,” in Proc. 16th Int. Bhurban Conf. Appl. Sci. Technol. (IBCAST), Islamabad, Pakistan, Jan. 2019, pp. 463-468. [12] S. Sabour, N. Frosst, and G. E. Hinton, “Dynamic routing between capsules,” in Proc. Int. Conf. Neural Inf. Process. Syst., 2017, pp. 3856- 3866. [13] A. Vaswani et al., “Attention is all you need,” in Proc. Int. Conf. Neural Inf. Process. Syst., 2017, pp. 5998-6008. [14] E. Chemali, P. J. Kollmeyer, M. Preindl, R. Ahmed, and A. Emadi, “Long short-term memory networks for accurate state-of-charge estima- tion of Li-ion batteries,” IEEE Trans. Ind. Electron., vol. 65, no. 8, pp. 6730-6739, Aug. 2018. [15] E. Chemali, P. J. Kollmeyer, M. Preindl, and A. Emadi, “State-of-charge estimation of Li-ion batteries using deep neural networks: A machine learning approach,” J. Power Sources, vol. 400, pp. 242-255, 2018.", "page": 5, "position": 2}
{"chunk_id": "2304.06043v1_p6_c0", "doc_id": "2304.06043v1", "text": "7 (a) (b) (c) Fig. 14: Mean Absolute Error (MAE) metrics obtained for Voltage and battery capacity data [19] for various horizon values for DeepAR (a), N-BEATS (b) and DeepTCN (c) (a) (b) (c) Fig. 15: Mean Absolute Error (MAE) metrics obtained for Voltage and battery capacity data [18] for various horizon values for DeepAR (a), N-BEATS (b) and DeepTCN (c) [16] B. Hanin, “Universal function approximation by deep neural nets with bounded width and ReLU activations,” Mathemat., vol. 7, no. 10, 2019, Art. no. 992. [17] F. Naaz, J. Channegowda, M. Lakshminarayanan, N. S. John and A. Herle, “Solving Limited Data Challenges in Battery Parameter Estima- tors by Using Generative Adversarial Networks,” 2021 IEEE PES/IAS PowerAfrica, 2021, pp. 1-3, [18] Kollmeyer, Phillip (2018), “Panasonic 18650PF Li-ion Battery Data”, Mendeley Data, V1, doi: 10.17632/wykht8y7tg.1 [19] Diao, W., Saxena, S., Pecht, M. Accelerated Cycle Life Testing and Capacity Degradation Modeling of LiCoO2 -graphite Cells. J. Power Sources 2019, 435, 226830. [20] V. Flunkert, D. Salinas and J. Gasthaus, “DeepAR: Probabilistic forecast- ing with autoregressive recurrent networks”, 2017, [online] Available: https://arxiv.org/abs/1704.04110. [21] Guoqiang Zhang, B. Eddy Patuwo, and Michael Y. Hu. Forecasting with artiﬁcial neural networks:: The state of the art. International Journal of Forecasting, 14(1):35-62, 1998. [22] F. A. Gers, D. Eck, and J. Schmidhuber. Applying LSTM to time series predictable through time-window approaches. In Georg Dorffner, editor, Artiﬁcial Neural Networks - ICANN 2001 (Proceedings), pages 669- 676. Springer, 2001. [23] Nikolaos Kourentzes. Intermittent demand forecasts with neural net- works. International Journal of Production Economics, 143(1):198-206, 2013. ISSN 09255273. doi: 10.1016/j.ijpe.2013.01.009. [24] Iebeling Kaastra and Milton Boyd. Designing a neural network for fore- casting ﬁnancial and economic time series. Neurocomputing, 10(3):215- 236, 1996. [25] M Ghiassi, H Saidane, and DK Zimbra. A dynamic artiﬁcial neural network model for forecasting time series events. International Journal of Forecasting, 21(2):341-362, 2005. [26] Alex Graves. Generating sequences with recurrent neural networks. arXiv:1308.0850, 2013. [27] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104-3112, 2014. [28] B. N. Oreshkin, D. Carpov,", "page": 6, "position": 0}
